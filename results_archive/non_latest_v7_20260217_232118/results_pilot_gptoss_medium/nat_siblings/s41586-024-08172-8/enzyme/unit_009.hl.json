{
  "schema_version": "0.4",
  "ir_kind": "hl",
  "metadata": {
    "title": "Train scPoli model",
    "source": {
      "type": "paper",
      "id": "s41586-024-08172-8.pdf"
    }
  },
  "resources": {
    "materials": [],
    "containers": [],
    "equipment": [],
    "samples": [],
    "data": []
  },
  "protocol": {
    "steps": [
      {
        "id": "s1",
        "op": "annotate",
        "inputs": [],
        "outputs": [],
        "params": {},
        "annotations": {
          "note": "We chose the hidden layer size of the one-layer scPoli encoder and decoder as 1,024, and the latent embedding dimension as ten. We used a value of 100 for the ‘alpha_epoch_anneal’ parameter. We did not use the unlabelled prototype pretraining. We trained the model for a total of seven epochs, five of which were pretraining epochs."
        }
      }
    ],
    "start_step_id": "s1",
    "edges": [
      {
        "from": "s1",
        "to": "s1"
      }
    ],
    "detail_level": 0
  }
}