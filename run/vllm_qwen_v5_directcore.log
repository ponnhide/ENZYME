(APIServer pid=2355576) INFO 02-17 00:31:49 [utils.py:325] 
(APIServer pid=2355576) INFO 02-17 00:31:49 [utils.py:325]        █     █     █▄   ▄█
(APIServer pid=2355576) INFO 02-17 00:31:49 [utils.py:325]  ▄▄ ▄█ █     █     █ ▀▄▀ █  version 0.15.1
(APIServer pid=2355576) INFO 02-17 00:31:49 [utils.py:325]   █▄█▀ █     █     █     █  model   /users/hideto/Project/vLLM/FP8/Qwen3-Next-80B-A3B-Instruct-FP8
(APIServer pid=2355576) INFO 02-17 00:31:49 [utils.py:325]    ▀▀  ▀▀▀▀▀ ▀▀▀▀▀ ▀     ▀
(APIServer pid=2355576) INFO 02-17 00:31:49 [utils.py:325] 
(APIServer pid=2355576) INFO 02-17 00:31:49 [utils.py:261] non-default args: {'model_tag': '/users/hideto/Project/vLLM/FP8/Qwen3-Next-80B-A3B-Instruct-FP8', 'api_server_count': 1, 'host': '0.0.0.0', 'model': '/users/hideto/Project/vLLM/FP8/Qwen3-Next-80B-A3B-Instruct-FP8', 'max_model_len': 32768, 'served_model_name': ['qwen3-next-80b-a3b-instruct-fp8-v5'], 'generation_config': 'vllm', 'reasoning_parser': 'qwen3', 'tensor_parallel_size': 4, 'gpu_memory_utilization': 0.8, 'max_num_batched_tokens': 512, 'max_num_seqs': 4}
(APIServer pid=2355576) INFO 02-17 00:31:49 [model.py:541] Resolved architecture: Qwen3NextForCausalLM
(APIServer pid=2355576) INFO 02-17 00:31:49 [model.py:1561] Using max model len 32768
(APIServer pid=2355576) INFO 02-17 00:31:50 [scheduler.py:226] Chunked prefill is enabled with max_num_batched_tokens=512.
(APIServer pid=2355576) INFO 02-17 00:31:50 [config.py:504] Setting attention block size to 272 tokens to ensure that attention page size is >= mamba page size.
(APIServer pid=2355576) INFO 02-17 00:31:50 [config.py:535] Padding mamba page size by 1.49% to ensure that mamba page size and attention page size are exactly equal.
(APIServer pid=2355576) INFO 02-17 00:31:50 [vllm.py:624] Asynchronous scheduling is enabled.
(EngineCore_DP0 pid=2355895) INFO 02-17 00:32:02 [core.py:96] Initializing a V1 LLM engine (v0.15.1) with config: model='/users/hideto/Project/vLLM/FP8/Qwen3-Next-80B-A3B-Instruct-FP8', speculative_config=None, tokenizer='/users/hideto/Project/vLLM/FP8/Qwen3-Next-80B-A3B-Instruct-FP8', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=4, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=fp8, enforce_eager=False, enable_return_routed_experts=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='qwen3', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=qwen3-next-80b-a3b-instruct-fp8-v5, enable_prefix_caching=False, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['+quant_fp8', 'none', '+quant_fp8'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer', 'vllm::rocm_aiter_sparse_attn_indexer', 'vllm::unified_kv_cache_update'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [512], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': True, 'fuse_act_quant': True, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 8, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None, 'static_all_moe_layers': []}
(EngineCore_DP0 pid=2355895) WARNING 02-17 00:32:02 [multiproc_executor.py:910] Reducing Torch parallelism from 96 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
INFO 02-17 00:32:14 [parallel_state.py:1212] world_size=4 rank=1 local_rank=1 distributed_init_method=tcp://127.0.0.1:41915 backend=nccl
INFO 02-17 00:32:14 [parallel_state.py:1212] world_size=4 rank=2 local_rank=2 distributed_init_method=tcp://127.0.0.1:41915 backend=nccl
INFO 02-17 00:32:14 [parallel_state.py:1212] world_size=4 rank=3 local_rank=3 distributed_init_method=tcp://127.0.0.1:41915 backend=nccl
INFO 02-17 00:32:14 [parallel_state.py:1212] world_size=4 rank=0 local_rank=0 distributed_init_method=tcp://127.0.0.1:41915 backend=nccl
INFO 02-17 00:32:15 [pynccl.py:111] vLLM is using nccl==2.27.5
WARNING 02-17 00:32:15 [symm_mem.py:67] SymmMemCommunicator: Device capability 8.9 not supported, communicator is not available.
WARNING 02-17 00:32:15 [symm_mem.py:67] SymmMemCommunicator: Device capability 8.9 not supported, communicator is not available.
WARNING 02-17 00:32:15 [symm_mem.py:67] SymmMemCommunicator: Device capability 8.9 not supported, communicator is not available.
WARNING 02-17 00:32:15 [symm_mem.py:67] SymmMemCommunicator: Device capability 8.9 not supported, communicator is not available.
WARNING 02-17 00:32:15 [custom_all_reduce.py:154] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
WARNING 02-17 00:32:15 [custom_all_reduce.py:154] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
WARNING 02-17 00:32:15 [custom_all_reduce.py:154] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
WARNING 02-17 00:32:15 [custom_all_reduce.py:154] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
INFO 02-17 00:32:15 [parallel_state.py:1423] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 1, EP rank 1
INFO 02-17 00:32:15 [parallel_state.py:1423] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
INFO 02-17 00:32:15 [parallel_state.py:1423] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 2, EP rank 2
INFO 02-17 00:32:15 [parallel_state.py:1423] rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 3, EP rank 3
(Worker_TP0 pid=2356192) INFO 02-17 00:32:16 [gpu_model_runner.py:4033] Starting to load model /users/hideto/Project/vLLM/FP8/Qwen3-Next-80B-A3B-Instruct-FP8...
(Worker_TP0 pid=2356192) INFO 02-17 00:32:17 [fp8.py:328] Using TRITON Fp8 MoE backend out of potential backends: ['AITER', 'FLASHINFER_TRTLLM', 'FLASHINFER_CUTLASS', 'DEEPGEMM', 'BATCHED_DEEPGEMM', 'TRITON', 'BATCHED_TRITON', 'MARLIN'].
(Worker_TP0 pid=2356192) INFO 02-17 00:32:17 [cuda.py:364] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
(Worker_TP0 pid=2356192) Loading safetensors checkpoint shards:   0% Completed | 0/8 [00:00<?, ?it/s]
(Worker_TP0 pid=2356192) Loading safetensors checkpoint shards:  12% Completed | 1/8 [00:01<00:11,  1.60s/it]
(Worker_TP0 pid=2356192) Loading safetensors checkpoint shards:  25% Completed | 2/8 [00:04<00:15,  2.63s/it]
(Worker_TP0 pid=2356192) Loading safetensors checkpoint shards:  38% Completed | 3/8 [00:08<00:15,  3.02s/it]
(Worker_TP0 pid=2356192) Loading safetensors checkpoint shards:  50% Completed | 4/8 [00:11<00:12,  3.06s/it]
(Worker_TP0 pid=2356192) Loading safetensors checkpoint shards:  62% Completed | 5/8 [00:14<00:09,  3.18s/it]
(Worker_TP0 pid=2356192) Loading safetensors checkpoint shards:  75% Completed | 6/8 [00:18<00:06,  3.27s/it]
(Worker_TP0 pid=2356192) Loading safetensors checkpoint shards:  88% Completed | 7/8 [00:21<00:03,  3.34s/it]
(Worker_TP0 pid=2356192) Loading safetensors checkpoint shards: 100% Completed | 8/8 [00:25<00:00,  3.37s/it]
(Worker_TP0 pid=2356192) Loading safetensors checkpoint shards: 100% Completed | 8/8 [00:25<00:00,  3.16s/it]
(Worker_TP0 pid=2356192) 
(Worker_TP0 pid=2356192) INFO 02-17 00:32:43 [default_loader.py:291] Loading weights took 25.58 seconds
(Worker_TP0 pid=2356192) INFO 02-17 00:32:44 [gpu_model_runner.py:4130] Model loading took 18.83 GiB memory and 26.770724 seconds
(Worker_TP0 pid=2356192) INFO 02-17 00:32:57 [backends.py:812] Using cache directory: /users/hideto/.cache/vllm/torch_compile_cache/cd367568c8/rank_0_0/backbone for vLLM's torch.compile
(Worker_TP0 pid=2356192) INFO 02-17 00:32:57 [backends.py:872] Dynamo bytecode transform time: 13.24 s
(Worker_TP0 pid=2356192) WARNING 02-17 00:33:05 [fp8_utils.py:1175] Using default W8A8 Block FP8 kernel config. Performance might be sub-optimal! Config file not found at /users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/model_executor/layers/quantization/utils/configs/N=3072,K=2048,device_name=NVIDIA_RTX_6000_Ada_Generation,dtype=fp8_w8a8,block_shape=[128,128].json
(Worker_TP1 pid=2356193) WARNING 02-17 00:33:05 [fp8_utils.py:1175] Using default W8A8 Block FP8 kernel config. Performance might be sub-optimal! Config file not found at /users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/model_executor/layers/quantization/utils/configs/N=3072,K=2048,device_name=NVIDIA_RTX_6000_Ada_Generation,dtype=fp8_w8a8,block_shape=[128,128].json
(Worker_TP3 pid=2356195) WARNING 02-17 00:33:05 [fp8_utils.py:1175] Using default W8A8 Block FP8 kernel config. Performance might be sub-optimal! Config file not found at /users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/model_executor/layers/quantization/utils/configs/N=3072,K=2048,device_name=NVIDIA_RTX_6000_Ada_Generation,dtype=fp8_w8a8,block_shape=[128,128].json
(Worker_TP2 pid=2356194) WARNING 02-17 00:33:05 [fp8_utils.py:1175] Using default W8A8 Block FP8 kernel config. Performance might be sub-optimal! Config file not found at /users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/model_executor/layers/quantization/utils/configs/N=3072,K=2048,device_name=NVIDIA_RTX_6000_Ada_Generation,dtype=fp8_w8a8,block_shape=[128,128].json
(Worker_TP2 pid=2356194) WARNING 02-17 00:33:06 [fp8_utils.py:1175] Using default W8A8 Block FP8 kernel config. Performance might be sub-optimal! Config file not found at /users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/model_executor/layers/quantization/utils/configs/N=2048,K=1024,device_name=NVIDIA_RTX_6000_Ada_Generation,dtype=fp8_w8a8,block_shape=[128,128].json
(Worker_TP1 pid=2356193) WARNING 02-17 00:33:06 [fp8_utils.py:1175] Using default W8A8 Block FP8 kernel config. Performance might be sub-optimal! Config file not found at /users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/model_executor/layers/quantization/utils/configs/N=2048,K=1024,device_name=NVIDIA_RTX_6000_Ada_Generation,dtype=fp8_w8a8,block_shape=[128,128].json
(Worker_TP0 pid=2356192) WARNING 02-17 00:33:06 [fp8_utils.py:1175] Using default W8A8 Block FP8 kernel config. Performance might be sub-optimal! Config file not found at /users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/model_executor/layers/quantization/utils/configs/N=2048,K=1024,device_name=NVIDIA_RTX_6000_Ada_Generation,dtype=fp8_w8a8,block_shape=[128,128].json
(Worker_TP3 pid=2356195) WARNING 02-17 00:33:06 [fp8_utils.py:1175] Using default W8A8 Block FP8 kernel config. Performance might be sub-optimal! Config file not found at /users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/model_executor/layers/quantization/utils/configs/N=2048,K=1024,device_name=NVIDIA_RTX_6000_Ada_Generation,dtype=fp8_w8a8,block_shape=[128,128].json
(Worker_TP2 pid=2356194) WARNING 02-17 00:33:06 [fp8_utils.py:1175] Using default W8A8 Block FP8 kernel config. Performance might be sub-optimal! Config file not found at /users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/model_executor/layers/quantization/utils/configs/N=256,K=2048,device_name=NVIDIA_RTX_6000_Ada_Generation,dtype=fp8_w8a8,block_shape=[128,128].json
(Worker_TP0 pid=2356192) WARNING 02-17 00:33:06 [fp8_utils.py:1175] Using default W8A8 Block FP8 kernel config. Performance might be sub-optimal! Config file not found at /users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/model_executor/layers/quantization/utils/configs/N=256,K=2048,device_name=NVIDIA_RTX_6000_Ada_Generation,dtype=fp8_w8a8,block_shape=[128,128].json
(Worker_TP1 pid=2356193) WARNING 02-17 00:33:06 [fp8_utils.py:1175] Using default W8A8 Block FP8 kernel config. Performance might be sub-optimal! Config file not found at /users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/model_executor/layers/quantization/utils/configs/N=256,K=2048,device_name=NVIDIA_RTX_6000_Ada_Generation,dtype=fp8_w8a8,block_shape=[128,128].json
(Worker_TP3 pid=2356195) WARNING 02-17 00:33:06 [fp8_utils.py:1175] Using default W8A8 Block FP8 kernel config. Performance might be sub-optimal! Config file not found at /users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/model_executor/layers/quantization/utils/configs/N=256,K=2048,device_name=NVIDIA_RTX_6000_Ada_Generation,dtype=fp8_w8a8,block_shape=[128,128].json
(Worker_TP2 pid=2356194) WARNING 02-17 00:33:06 [fp8_utils.py:1175] Using default W8A8 Block FP8 kernel config. Performance might be sub-optimal! Config file not found at /users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/model_executor/layers/quantization/utils/configs/N=2048,K=128,device_name=NVIDIA_RTX_6000_Ada_Generation,dtype=fp8_w8a8,block_shape=[128,128].json
(Worker_TP1 pid=2356193) WARNING 02-17 00:33:06 [fp8_utils.py:1175] Using default W8A8 Block FP8 kernel config. Performance might be sub-optimal! Config file not found at /users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/model_executor/layers/quantization/utils/configs/N=2048,K=128,device_name=NVIDIA_RTX_6000_Ada_Generation,dtype=fp8_w8a8,block_shape=[128,128].json
(Worker_TP0 pid=2356192) WARNING 02-17 00:33:06 [fp8_utils.py:1175] Using default W8A8 Block FP8 kernel config. Performance might be sub-optimal! Config file not found at /users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/model_executor/layers/quantization/utils/configs/N=2048,K=128,device_name=NVIDIA_RTX_6000_Ada_Generation,dtype=fp8_w8a8,block_shape=[128,128].json
(Worker_TP3 pid=2356195) WARNING 02-17 00:33:06 [fp8_utils.py:1175] Using default W8A8 Block FP8 kernel config. Performance might be sub-optimal! Config file not found at /users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/model_executor/layers/quantization/utils/configs/N=2048,K=128,device_name=NVIDIA_RTX_6000_Ada_Generation,dtype=fp8_w8a8,block_shape=[128,128].json
(Worker_TP0 pid=2356192) WARNING 02-17 00:33:06 [fused_moe.py:1090] Using default MoE config. Performance might be sub-optimal! Config file not found at /users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/model_executor/layers/fused_moe/configs/E=512,N=128,device_name=NVIDIA_RTX_6000_Ada_Generation,dtype=fp8_w8a8,block_shape=[128,128].json
(Worker_TP0 pid=2356192) WARNING 02-17 00:33:06 [fp8_utils.py:1175] Using default W8A8 Block FP8 kernel config. Performance might be sub-optimal! Config file not found at /users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/model_executor/layers/quantization/utils/configs/N=2560,K=2048,device_name=NVIDIA_RTX_6000_Ada_Generation,dtype=fp8_w8a8,block_shape=[128,128].json
(Worker_TP2 pid=2356194) WARNING 02-17 00:33:06 [fp8_utils.py:1175] Using default W8A8 Block FP8 kernel config. Performance might be sub-optimal! Config file not found at /users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/model_executor/layers/quantization/utils/configs/N=2560,K=2048,device_name=NVIDIA_RTX_6000_Ada_Generation,dtype=fp8_w8a8,block_shape=[128,128].json
(Worker_TP1 pid=2356193) WARNING 02-17 00:33:06 [fp8_utils.py:1175] Using default W8A8 Block FP8 kernel config. Performance might be sub-optimal! Config file not found at /users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/model_executor/layers/quantization/utils/configs/N=2560,K=2048,device_name=NVIDIA_RTX_6000_Ada_Generation,dtype=fp8_w8a8,block_shape=[128,128].json
(Worker_TP3 pid=2356195) WARNING 02-17 00:33:06 [fp8_utils.py:1175] Using default W8A8 Block FP8 kernel config. Performance might be sub-optimal! Config file not found at /users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/model_executor/layers/quantization/utils/configs/N=2560,K=2048,device_name=NVIDIA_RTX_6000_Ada_Generation,dtype=fp8_w8a8,block_shape=[128,128].json
(Worker_TP1 pid=2356193) INFO 02-17 00:33:08 [backends.py:267] Directly load the compiled graph(s) for compile range (1, 512) from the cache, took 2.820 s
(Worker_TP2 pid=2356194) INFO 02-17 00:33:08 [backends.py:267] Directly load the compiled graph(s) for compile range (1, 512) from the cache, took 2.760 s
(Worker_TP3 pid=2356195) INFO 02-17 00:33:08 [backends.py:267] Directly load the compiled graph(s) for compile range (1, 512) from the cache, took 2.771 s
(Worker_TP0 pid=2356192) INFO 02-17 00:33:08 [backends.py:267] Directly load the compiled graph(s) for compile range (1, 512) from the cache, took 2.813 s
(Worker_TP0 pid=2356192) INFO 02-17 00:33:08 [monitor.py:34] torch.compile takes 16.05 s in total
(Worker_TP0 pid=2356192) INFO 02-17 00:33:09 [gpu_worker.py:356] Available KV cache memory: 18.97 GiB
(EngineCore_DP0 pid=2355895) INFO 02-17 00:33:09 [kv_cache_utils.py:1307] GPU KV cache size: 414,256 tokens
(EngineCore_DP0 pid=2355895) INFO 02-17 00:33:09 [kv_cache_utils.py:1312] Maximum concurrency for 32,768 tokens per request: 49.14x
(Worker_TP0 pid=2356192) Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/4 [00:00<?, ?it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  25%|██▌       | 1/4 [00:00<00:00,  3.85it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 2/4 [00:00<00:00,  4.05it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|███████▌  | 3/4 [00:00<00:00,  4.31it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:01<00:00,  3.59it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:01<00:00,  3.76it/s]
(Worker_TP0 pid=2356192) Capturing CUDA graphs (decode, FULL):   0%|          | 0/3 [00:00<?, ?it/s]Capturing CUDA graphs (decode, FULL):  33%|███▎      | 1/3 [00:00<00:00,  3.34it/s]Capturing CUDA graphs (decode, FULL):  67%|██████▋   | 2/3 [00:00<00:00,  3.80it/s]Capturing CUDA graphs (decode, FULL): 100%|██████████| 3/3 [00:00<00:00,  3.96it/s]Capturing CUDA graphs (decode, FULL): 100%|██████████| 3/3 [00:00<00:00,  3.86it/s]
(Worker_TP0 pid=2356192) INFO 02-17 00:33:12 [gpu_model_runner.py:5063] Graph capturing finished in 3 secs, took 0.29 GiB
(EngineCore_DP0 pid=2355895) INFO 02-17 00:33:12 [core.py:272] init engine (profile, create kv cache, warmup model) took 28.24 seconds
(EngineCore_DP0 pid=2355895) INFO 02-17 00:33:13 [vllm.py:624] Asynchronous scheduling is enabled.
(APIServer pid=2355576) INFO 02-17 00:33:13 [api_server.py:665] Supported tasks: ['generate']
(APIServer pid=2355576) INFO 02-17 00:33:13 [serving.py:177] Warming up chat template processing...
(APIServer pid=2355576) INFO 02-17 00:33:13 [hf.py:310] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.
(APIServer pid=2355576) INFO 02-17 00:33:13 [serving.py:212] Chat template warmup completed in 37.2ms
(APIServer pid=2355576) INFO 02-17 00:33:13 [api_server.py:946] Starting vLLM API server 0 on http://0.0.0.0:8000
(APIServer pid=2355576) INFO 02-17 00:33:13 [launcher.py:38] Available routes are:
(APIServer pid=2355576) INFO 02-17 00:33:13 [launcher.py:46] Route: /openapi.json, Methods: HEAD, GET
(APIServer pid=2355576) INFO 02-17 00:33:13 [launcher.py:46] Route: /docs, Methods: HEAD, GET
(APIServer pid=2355576) INFO 02-17 00:33:13 [launcher.py:46] Route: /docs/oauth2-redirect, Methods: HEAD, GET
(APIServer pid=2355576) INFO 02-17 00:33:13 [launcher.py:46] Route: /redoc, Methods: HEAD, GET
(APIServer pid=2355576) INFO 02-17 00:33:13 [launcher.py:46] Route: /scale_elastic_ep, Methods: POST
(APIServer pid=2355576) INFO 02-17 00:33:13 [launcher.py:46] Route: /is_scaling_elastic_ep, Methods: POST
(APIServer pid=2355576) INFO 02-17 00:33:13 [launcher.py:46] Route: /tokenize, Methods: POST
(APIServer pid=2355576) INFO 02-17 00:33:13 [launcher.py:46] Route: /detokenize, Methods: POST
(APIServer pid=2355576) INFO 02-17 00:33:13 [launcher.py:46] Route: /inference/v1/generate, Methods: POST
(APIServer pid=2355576) INFO 02-17 00:33:13 [launcher.py:46] Route: /pause, Methods: POST
(APIServer pid=2355576) INFO 02-17 00:33:13 [launcher.py:46] Route: /resume, Methods: POST
(APIServer pid=2355576) INFO 02-17 00:33:13 [launcher.py:46] Route: /is_paused, Methods: GET
(APIServer pid=2355576) INFO 02-17 00:33:13 [launcher.py:46] Route: /metrics, Methods: GET
(APIServer pid=2355576) INFO 02-17 00:33:13 [launcher.py:46] Route: /health, Methods: GET
(APIServer pid=2355576) INFO 02-17 00:33:13 [launcher.py:46] Route: /v1/chat/completions, Methods: POST
(APIServer pid=2355576) INFO 02-17 00:33:13 [launcher.py:46] Route: /v1/chat/completions/render, Methods: POST
(APIServer pid=2355576) INFO 02-17 00:33:13 [launcher.py:46] Route: /v1/responses, Methods: POST
(APIServer pid=2355576) INFO 02-17 00:33:13 [launcher.py:46] Route: /v1/responses/{response_id}, Methods: GET
(APIServer pid=2355576) INFO 02-17 00:33:13 [launcher.py:46] Route: /v1/responses/{response_id}/cancel, Methods: POST
(APIServer pid=2355576) INFO 02-17 00:33:13 [launcher.py:46] Route: /v1/audio/transcriptions, Methods: POST
(APIServer pid=2355576) INFO 02-17 00:33:13 [launcher.py:46] Route: /v1/audio/translations, Methods: POST
(APIServer pid=2355576) INFO 02-17 00:33:13 [launcher.py:46] Route: /v1/completions, Methods: POST
(APIServer pid=2355576) INFO 02-17 00:33:13 [launcher.py:46] Route: /v1/completions/render, Methods: POST
(APIServer pid=2355576) INFO 02-17 00:33:13 [launcher.py:46] Route: /v1/messages, Methods: POST
(APIServer pid=2355576) INFO 02-17 00:33:13 [launcher.py:46] Route: /v1/models, Methods: GET
(APIServer pid=2355576) INFO 02-17 00:33:13 [launcher.py:46] Route: /load, Methods: GET
(APIServer pid=2355576) INFO 02-17 00:33:13 [launcher.py:46] Route: /version, Methods: GET
(APIServer pid=2355576) INFO 02-17 00:33:13 [launcher.py:46] Route: /ping, Methods: GET
(APIServer pid=2355576) INFO 02-17 00:33:13 [launcher.py:46] Route: /ping, Methods: POST
(APIServer pid=2355576) INFO 02-17 00:33:13 [launcher.py:46] Route: /invocations, Methods: POST
(APIServer pid=2355576) INFO 02-17 00:33:13 [launcher.py:46] Route: /classify, Methods: POST
(APIServer pid=2355576) INFO 02-17 00:33:13 [launcher.py:46] Route: /v1/embeddings, Methods: POST
(APIServer pid=2355576) INFO 02-17 00:33:13 [launcher.py:46] Route: /score, Methods: POST
(APIServer pid=2355576) INFO 02-17 00:33:13 [launcher.py:46] Route: /v1/score, Methods: POST
(APIServer pid=2355576) INFO 02-17 00:33:13 [launcher.py:46] Route: /rerank, Methods: POST
(APIServer pid=2355576) INFO 02-17 00:33:13 [launcher.py:46] Route: /v1/rerank, Methods: POST
(APIServer pid=2355576) INFO 02-17 00:33:13 [launcher.py:46] Route: /v2/rerank, Methods: POST
(APIServer pid=2355576) INFO 02-17 00:33:13 [launcher.py:46] Route: /pooling, Methods: POST
(APIServer pid=2355576) INFO:     Started server process [2355576]
(APIServer pid=2355576) INFO:     Waiting for application startup.
(APIServer pid=2355576) INFO:     Application startup complete.
(APIServer pid=2355576) INFO:     127.0.0.1:59712 - "GET /v1/models HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:33:14 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) WARNING 02-17 00:33:14 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) WARNING 02-17 00:33:14 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) WARNING 02-17 00:33:14 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO 02-17 00:33:34 [loggers.py:257] Engine 000: Avg prompt throughput: 2211.4 tokens/s, Avg generation throughput: 5.0 tokens/s, Running: 3 reqs, Waiting: 1 reqs, GPU KV cache usage: 2.0%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) INFO 02-17 00:33:44 [loggers.py:257] Engine 000: Avg prompt throughput: 2329.7 tokens/s, Avg generation throughput: 270.6 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 3.1%, Prefix cache hit rate: 0.0%
(Worker_TP2 pid=2356194) /users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/xgrammar/kernels/apply_token_bitmask_inplace_triton.py:109: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
(Worker_TP3 pid=2356195) /users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/xgrammar/kernels/apply_token_bitmask_inplace_triton.py:109: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
(Worker_TP1 pid=2356193) /users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/xgrammar/kernels/apply_token_bitmask_inplace_triton.py:109: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
(Worker_TP0 pid=2356192) /users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/xgrammar/kernels/apply_token_bitmask_inplace_triton.py:109: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
(Worker_TP2 pid=2356194)   indices_cpu = torch.tensor(indices, dtype=torch.int32)
(Worker_TP3 pid=2356195)   indices_cpu = torch.tensor(indices, dtype=torch.int32)
(Worker_TP1 pid=2356193)   indices_cpu = torch.tensor(indices, dtype=torch.int32)
(Worker_TP0 pid=2356192)   indices_cpu = torch.tensor(indices, dtype=torch.int32)
(APIServer pid=2355576) INFO:     127.0.0.1:59740 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:33:46 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:59756 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:33:50 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO 02-17 00:33:54 [loggers.py:257] Engine 000: Avg prompt throughput: 152.8 tokens/s, Avg generation throughput: 429.2 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.9%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) INFO:     127.0.0.1:33750 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:33:55 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:33762 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:33:58 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:38360 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:34:03 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO 02-17 00:34:04 [loggers.py:257] Engine 000: Avg prompt throughput: 315.7 tokens/s, Avg generation throughput: 372.6 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.1%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) INFO:     127.0.0.1:59724 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:34:06 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:39894 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) INFO:     127.0.0.1:38344 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:34:08 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) WARNING 02-17 00:34:09 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:39930 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) INFO:     127.0.0.1:39920 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:34:13 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO 02-17 00:34:14 [loggers.py:257] Engine 000: Avg prompt throughput: 344.8 tokens/s, Avg generation throughput: 348.4 tokens/s, Running: 3 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.2%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) WARNING 02-17 00:34:14 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:45706 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) INFO:     127.0.0.1:59718 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:34:17 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) WARNING 02-17 00:34:17 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:45716 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) INFO:     127.0.0.1:45710 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:34:21 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) WARNING 02-17 00:34:22 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:45724 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) INFO 02-17 00:34:24 [loggers.py:257] Engine 000: Avg prompt throughput: 425.0 tokens/s, Avg generation throughput: 352.2 tokens/s, Running: 3 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.5%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) WARNING 02-17 00:34:25 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:58670 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:34:28 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:58664 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:34:29 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:39904 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:34:30 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:58682 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:34:33 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO 02-17 00:34:34 [loggers.py:257] Engine 000: Avg prompt throughput: 591.7 tokens/s, Avg generation throughput: 330.0 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.8%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) INFO:     127.0.0.1:45714 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) INFO:     127.0.0.1:38898 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:34:36 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) WARNING 02-17 00:34:37 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:38926 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) INFO:     127.0.0.1:58690 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:34:41 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) WARNING 02-17 00:34:41 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO 02-17 00:34:44 [loggers.py:257] Engine 000: Avg prompt throughput: 416.2 tokens/s, Avg generation throughput: 355.2 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.7%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) INFO:     127.0.0.1:38946 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) INFO:     127.0.0.1:38942 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:34:47 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) WARNING 02-17 00:34:48 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:38912 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) INFO:     127.0.0.1:59516 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:34:52 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) WARNING 02-17 00:34:52 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO 02-17 00:34:54 [loggers.py:257] Engine 000: Avg prompt throughput: 1062.0 tokens/s, Avg generation throughput: 300.9 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.2%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) INFO:     127.0.0.1:54700 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:34:58 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:58704 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) INFO 02-17 00:35:04 [loggers.py:257] Engine 000: Avg prompt throughput: 82.6 tokens/s, Avg generation throughput: 422.7 tokens/s, Running: 3 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.0%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) WARNING 02-17 00:35:04 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:54706 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:35:05 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:59530 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:35:09 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:54704 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) INFO:     127.0.0.1:40280 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:35:11 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) WARNING 02-17 00:35:11 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:40304 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:35:13 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO 02-17 00:35:14 [loggers.py:257] Engine 000: Avg prompt throughput: 651.4 tokens/s, Avg generation throughput: 338.9 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.6%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) INFO:     127.0.0.1:40292 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:35:16 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:36670 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) INFO:     127.0.0.1:40318 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:35:20 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) WARNING 02-17 00:35:21 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:36684 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:35:22 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO 02-17 00:35:24 [loggers.py:257] Engine 000: Avg prompt throughput: 369.5 tokens/s, Avg generation throughput: 367.1 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.6%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) INFO:     127.0.0.1:36688 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:35:26 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:36700 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) INFO:     127.0.0.1:40316 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:35:28 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) WARNING 02-17 00:35:28 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:39124 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:35:32 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) ERROR 02-17 00:35:32 [serving.py:323] Error in preprocessing prompt inputs
(APIServer pid=2355576) ERROR 02-17 00:35:32 [serving.py:323] Traceback (most recent call last):
(APIServer pid=2355576) ERROR 02-17 00:35:32 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/chat_completion/serving.py", line 301, in render_chat_request
(APIServer pid=2355576) ERROR 02-17 00:35:32 [serving.py:323]     conversation, engine_prompts = await self._preprocess_chat(
(APIServer pid=2355576) ERROR 02-17 00:35:32 [serving.py:323]                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=2355576) ERROR 02-17 00:35:32 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1219, in _preprocess_chat
(APIServer pid=2355576) ERROR 02-17 00:35:32 [serving.py:323]     engine_prompt = await self._tokenize_prompt_input_async(
(APIServer pid=2355576) ERROR 02-17 00:35:32 [serving.py:323]                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=2355576) ERROR 02-17 00:35:32 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1106, in _tokenize_prompt_input_async
(APIServer pid=2355576) ERROR 02-17 00:35:32 [serving.py:323]     async for result in self._tokenize_prompt_inputs_async(
(APIServer pid=2355576) ERROR 02-17 00:35:32 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1127, in _tokenize_prompt_inputs_async
(APIServer pid=2355576) ERROR 02-17 00:35:32 [serving.py:323]     yield await self._normalize_prompt_text_to_input(
(APIServer pid=2355576) ERROR 02-17 00:35:32 [serving.py:323]           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=2355576) ERROR 02-17 00:35:32 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 987, in _normalize_prompt_text_to_input
(APIServer pid=2355576) ERROR 02-17 00:35:32 [serving.py:323]     return self._validate_input(request, input_ids, input_text)
(APIServer pid=2355576) ERROR 02-17 00:35:32 [serving.py:323]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=2355576) ERROR 02-17 00:35:32 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1084, in _validate_input
(APIServer pid=2355576) ERROR 02-17 00:35:32 [serving.py:323]     raise VLLMValidationError(
(APIServer pid=2355576) ERROR 02-17 00:35:32 [serving.py:323] vllm.exceptions.VLLMValidationError: 'max_tokens' or 'max_completion_tokens' is too large: 16384. This model's maximum context length is 32768 tokens and your request has 21978 input tokens (16384 > 32768 - 21978). (parameter=max_tokens, value=16384)
(APIServer pid=2355576) INFO:     127.0.0.1:55318 - "POST /v1/chat/completions HTTP/1.1" 400 Bad Request
(APIServer pid=2355576) WARNING 02-17 00:35:32 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) ERROR 02-17 00:35:32 [serving.py:323] Error in preprocessing prompt inputs
(APIServer pid=2355576) ERROR 02-17 00:35:32 [serving.py:323] Traceback (most recent call last):
(APIServer pid=2355576) ERROR 02-17 00:35:32 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/chat_completion/serving.py", line 301, in render_chat_request
(APIServer pid=2355576) ERROR 02-17 00:35:32 [serving.py:323]     conversation, engine_prompts = await self._preprocess_chat(
(APIServer pid=2355576) ERROR 02-17 00:35:32 [serving.py:323]                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=2355576) ERROR 02-17 00:35:32 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1219, in _preprocess_chat
(APIServer pid=2355576) ERROR 02-17 00:35:32 [serving.py:323]     engine_prompt = await self._tokenize_prompt_input_async(
(APIServer pid=2355576) ERROR 02-17 00:35:32 [serving.py:323]                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=2355576) ERROR 02-17 00:35:32 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1106, in _tokenize_prompt_input_async
(APIServer pid=2355576) ERROR 02-17 00:35:32 [serving.py:323]     async for result in self._tokenize_prompt_inputs_async(
(APIServer pid=2355576) ERROR 02-17 00:35:32 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1127, in _tokenize_prompt_inputs_async
(APIServer pid=2355576) ERROR 02-17 00:35:32 [serving.py:323]     yield await self._normalize_prompt_text_to_input(
(APIServer pid=2355576) ERROR 02-17 00:35:32 [serving.py:323]           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=2355576) ERROR 02-17 00:35:32 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 987, in _normalize_prompt_text_to_input
(APIServer pid=2355576) ERROR 02-17 00:35:32 [serving.py:323]     return self._validate_input(request, input_ids, input_text)
(APIServer pid=2355576) ERROR 02-17 00:35:32 [serving.py:323]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=2355576) ERROR 02-17 00:35:32 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1084, in _validate_input
(APIServer pid=2355576) ERROR 02-17 00:35:32 [serving.py:323]     raise VLLMValidationError(
(APIServer pid=2355576) ERROR 02-17 00:35:32 [serving.py:323] vllm.exceptions.VLLMValidationError: 'max_tokens' or 'max_completion_tokens' is too large: 16384. This model's maximum context length is 32768 tokens and your request has 21978 input tokens (16384 > 32768 - 21978). (parameter=max_tokens, value=16384)
(APIServer pid=2355576) INFO:     127.0.0.1:55330 - "POST /v1/chat/completions HTTP/1.1" 400 Bad Request
(APIServer pid=2355576) WARNING 02-17 00:35:32 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) ERROR 02-17 00:35:32 [serving.py:323] Error in preprocessing prompt inputs
(APIServer pid=2355576) ERROR 02-17 00:35:32 [serving.py:323] Traceback (most recent call last):
(APIServer pid=2355576) ERROR 02-17 00:35:32 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/chat_completion/serving.py", line 301, in render_chat_request
(APIServer pid=2355576) ERROR 02-17 00:35:32 [serving.py:323]     conversation, engine_prompts = await self._preprocess_chat(
(APIServer pid=2355576) ERROR 02-17 00:35:32 [serving.py:323]                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=2355576) ERROR 02-17 00:35:32 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1219, in _preprocess_chat
(APIServer pid=2355576) ERROR 02-17 00:35:32 [serving.py:323]     engine_prompt = await self._tokenize_prompt_input_async(
(APIServer pid=2355576) ERROR 02-17 00:35:32 [serving.py:323]                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=2355576) ERROR 02-17 00:35:32 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1106, in _tokenize_prompt_input_async
(APIServer pid=2355576) ERROR 02-17 00:35:32 [serving.py:323]     async for result in self._tokenize_prompt_inputs_async(
(APIServer pid=2355576) ERROR 02-17 00:35:32 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1127, in _tokenize_prompt_inputs_async
(APIServer pid=2355576) ERROR 02-17 00:35:32 [serving.py:323]     yield await self._normalize_prompt_text_to_input(
(APIServer pid=2355576) ERROR 02-17 00:35:32 [serving.py:323]           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=2355576) ERROR 02-17 00:35:32 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 987, in _normalize_prompt_text_to_input
(APIServer pid=2355576) ERROR 02-17 00:35:32 [serving.py:323]     return self._validate_input(request, input_ids, input_text)
(APIServer pid=2355576) ERROR 02-17 00:35:32 [serving.py:323]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=2355576) ERROR 02-17 00:35:32 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1084, in _validate_input
(APIServer pid=2355576) ERROR 02-17 00:35:32 [serving.py:323]     raise VLLMValidationError(
(APIServer pid=2355576) ERROR 02-17 00:35:32 [serving.py:323] vllm.exceptions.VLLMValidationError: 'max_tokens' or 'max_completion_tokens' is too large: 16384. This model's maximum context length is 32768 tokens and your request has 21978 input tokens (16384 > 32768 - 21978). (parameter=max_tokens, value=16384)
(APIServer pid=2355576) INFO:     127.0.0.1:55338 - "POST /v1/chat/completions HTTP/1.1" 400 Bad Request
(APIServer pid=2355576) WARNING 02-17 00:35:32 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) ERROR 02-17 00:35:32 [serving.py:323] Error in preprocessing prompt inputs
(APIServer pid=2355576) ERROR 02-17 00:35:32 [serving.py:323] Traceback (most recent call last):
(APIServer pid=2355576) ERROR 02-17 00:35:32 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/chat_completion/serving.py", line 301, in render_chat_request
(APIServer pid=2355576) ERROR 02-17 00:35:32 [serving.py:323]     conversation, engine_prompts = await self._preprocess_chat(
(APIServer pid=2355576) ERROR 02-17 00:35:32 [serving.py:323]                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=2355576) ERROR 02-17 00:35:32 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1219, in _preprocess_chat
(APIServer pid=2355576) ERROR 02-17 00:35:32 [serving.py:323]     engine_prompt = await self._tokenize_prompt_input_async(
(APIServer pid=2355576) ERROR 02-17 00:35:32 [serving.py:323]                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=2355576) ERROR 02-17 00:35:32 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1106, in _tokenize_prompt_input_async
(APIServer pid=2355576) ERROR 02-17 00:35:32 [serving.py:323]     async for result in self._tokenize_prompt_inputs_async(
(APIServer pid=2355576) ERROR 02-17 00:35:32 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1127, in _tokenize_prompt_inputs_async
(APIServer pid=2355576) ERROR 02-17 00:35:32 [serving.py:323]     yield await self._normalize_prompt_text_to_input(
(APIServer pid=2355576) ERROR 02-17 00:35:32 [serving.py:323]           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=2355576) ERROR 02-17 00:35:32 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 987, in _normalize_prompt_text_to_input
(APIServer pid=2355576) ERROR 02-17 00:35:32 [serving.py:323]     return self._validate_input(request, input_ids, input_text)
(APIServer pid=2355576) ERROR 02-17 00:35:32 [serving.py:323]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=2355576) ERROR 02-17 00:35:32 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1084, in _validate_input
(APIServer pid=2355576) ERROR 02-17 00:35:32 [serving.py:323]     raise VLLMValidationError(
(APIServer pid=2355576) ERROR 02-17 00:35:32 [serving.py:323] vllm.exceptions.VLLMValidationError: 'max_tokens' or 'max_completion_tokens' is too large: 16384. This model's maximum context length is 32768 tokens and your request has 21978 input tokens (16384 > 32768 - 21978). (parameter=max_tokens, value=16384)
(APIServer pid=2355576) INFO:     127.0.0.1:55344 - "POST /v1/chat/completions HTTP/1.1" 400 Bad Request
(APIServer pid=2355576) WARNING 02-17 00:35:32 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) ERROR 02-17 00:35:32 [serving.py:323] Error in preprocessing prompt inputs
(APIServer pid=2355576) ERROR 02-17 00:35:32 [serving.py:323] Traceback (most recent call last):
(APIServer pid=2355576) ERROR 02-17 00:35:32 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/chat_completion/serving.py", line 301, in render_chat_request
(APIServer pid=2355576) ERROR 02-17 00:35:32 [serving.py:323]     conversation, engine_prompts = await self._preprocess_chat(
(APIServer pid=2355576) ERROR 02-17 00:35:32 [serving.py:323]                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=2355576) ERROR 02-17 00:35:32 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1219, in _preprocess_chat
(APIServer pid=2355576) ERROR 02-17 00:35:32 [serving.py:323]     engine_prompt = await self._tokenize_prompt_input_async(
(APIServer pid=2355576) ERROR 02-17 00:35:32 [serving.py:323]                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=2355576) ERROR 02-17 00:35:32 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1106, in _tokenize_prompt_input_async
(APIServer pid=2355576) ERROR 02-17 00:35:32 [serving.py:323]     async for result in self._tokenize_prompt_inputs_async(
(APIServer pid=2355576) ERROR 02-17 00:35:32 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1127, in _tokenize_prompt_inputs_async
(APIServer pid=2355576) ERROR 02-17 00:35:32 [serving.py:323]     yield await self._normalize_prompt_text_to_input(
(APIServer pid=2355576) ERROR 02-17 00:35:32 [serving.py:323]           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=2355576) ERROR 02-17 00:35:32 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 987, in _normalize_prompt_text_to_input
(APIServer pid=2355576) ERROR 02-17 00:35:32 [serving.py:323]     return self._validate_input(request, input_ids, input_text)
(APIServer pid=2355576) ERROR 02-17 00:35:32 [serving.py:323]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=2355576) ERROR 02-17 00:35:32 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1084, in _validate_input
(APIServer pid=2355576) ERROR 02-17 00:35:32 [serving.py:323]     raise VLLMValidationError(
(APIServer pid=2355576) ERROR 02-17 00:35:32 [serving.py:323] vllm.exceptions.VLLMValidationError: 'max_tokens' or 'max_completion_tokens' is too large: 16384. This model's maximum context length is 32768 tokens and your request has 21978 input tokens (16384 > 32768 - 21978). (parameter=max_tokens, value=16384)
(APIServer pid=2355576) INFO:     127.0.0.1:55360 - "POST /v1/chat/completions HTTP/1.1" 400 Bad Request
(APIServer pid=2355576) WARNING 02-17 00:35:32 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:39156 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) INFO:     127.0.0.1:39140 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:35:33 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) WARNING 02-17 00:35:34 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO 02-17 00:35:34 [loggers.py:257] Engine 000: Avg prompt throughput: 707.9 tokens/s, Avg generation throughput: 298.2 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.6%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) INFO:     127.0.0.1:39172 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:35:37 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:55394 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) INFO:     127.0.0.1:55382 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:35:41 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) WARNING 02-17 00:35:42 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO 02-17 00:35:44 [loggers.py:257] Engine 000: Avg prompt throughput: 396.1 tokens/s, Avg generation throughput: 389.4 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.8%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) INFO:     127.0.0.1:55376 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:35:44 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:55398 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) INFO:     127.0.0.1:58920 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:35:48 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) WARNING 02-17 00:35:48 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:55402 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:35:52 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO 02-17 00:35:54 [loggers.py:257] Engine 000: Avg prompt throughput: 553.7 tokens/s, Avg generation throughput: 362.4 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.7%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) INFO:     127.0.0.1:58946 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:35:57 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:57582 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:35:58 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:58930 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) INFO:     127.0.0.1:57594 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:36:03 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO 02-17 00:36:04 [loggers.py:257] Engine 000: Avg prompt throughput: 431.1 tokens/s, Avg generation throughput: 365.0 tokens/s, Running: 3 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.6%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) WARNING 02-17 00:36:04 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:57602 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:36:08 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:58956 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:36:11 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO 02-17 00:36:14 [loggers.py:257] Engine 000: Avg prompt throughput: 429.5 tokens/s, Avg generation throughput: 391.3 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.8%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) INFO:     127.0.0.1:42636 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:36:15 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:42640 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:36:22 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:57836 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) INFO 02-17 00:36:24 [loggers.py:257] Engine 000: Avg prompt throughput: 295.0 tokens/s, Avg generation throughput: 408.9 tokens/s, Running: 3 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.8%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) WARNING 02-17 00:36:25 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:50280 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:36:29 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:42648 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:36:32 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO 02-17 00:36:34 [loggers.py:257] Engine 000: Avg prompt throughput: 297.2 tokens/s, Avg generation throughput: 395.7 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.9%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) INFO:     127.0.0.1:50290 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:36:37 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:57844 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) INFO:     127.0.0.1:40296 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:36:40 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) WARNING 02-17 00:36:42 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO 02-17 00:36:44 [loggers.py:257] Engine 000: Avg prompt throughput: 276.3 tokens/s, Avg generation throughput: 374.7 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.9%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) INFO:     127.0.0.1:40284 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) INFO:     127.0.0.1:43728 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:36:48 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) WARNING 02-17 00:36:48 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO 02-17 00:36:54 [loggers.py:257] Engine 000: Avg prompt throughput: 235.7 tokens/s, Avg generation throughput: 422.6 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.1%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) INFO:     127.0.0.1:40298 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:36:54 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:43730 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) INFO:     127.0.0.1:42622 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:36:57 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) WARNING 02-17 00:36:59 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO 02-17 00:37:04 [loggers.py:257] Engine 000: Avg prompt throughput: 1189.5 tokens/s, Avg generation throughput: 307.4 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.2%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) INFO:     127.0.0.1:54434 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:37:09 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:54410 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) INFO 02-17 00:37:14 [loggers.py:257] Engine 000: Avg prompt throughput: 74.5 tokens/s, Avg generation throughput: 452.9 tokens/s, Running: 3 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.9%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) INFO:     127.0.0.1:47156 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:37:16 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) WARNING 02-17 00:37:16 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:39844 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:37:21 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO 02-17 00:37:24 [loggers.py:257] Engine 000: Avg prompt throughput: 817.6 tokens/s, Avg generation throughput: 308.0 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.5%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) INFO:     127.0.0.1:39858 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:37:28 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:39836 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:37:30 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:35920 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:37:32 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO 02-17 00:37:34 [loggers.py:257] Engine 000: Avg prompt throughput: 214.7 tokens/s, Avg generation throughput: 387.7 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.3%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) INFO:     127.0.0.1:35936 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) INFO:     127.0.0.1:47474 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:37:36 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:43736 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:37:36 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) WARNING 02-17 00:37:38 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO 02-17 00:37:44 [loggers.py:257] Engine 000: Avg prompt throughput: 257.0 tokens/s, Avg generation throughput: 190.8 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.1%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) INFO:     127.0.0.1:47480 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:37:46 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:47496 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:37:48 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:58548 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) INFO:     127.0.0.1:58544 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:37:51 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) WARNING 02-17 00:37:52 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO 02-17 00:37:54 [loggers.py:257] Engine 000: Avg prompt throughput: 338.4 tokens/s, Avg generation throughput: 361.7 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.2%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) INFO:     127.0.0.1:33758 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:37:57 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:58552 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:38:00 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:47504 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:38:01 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:33760 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) INFO 02-17 00:38:04 [loggers.py:257] Engine 000: Avg prompt throughput: 429.5 tokens/s, Avg generation throughput: 362.7 tokens/s, Running: 3 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.2%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) INFO:     127.0.0.1:33766 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:38:05 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) WARNING 02-17 00:38:06 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:43116 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:38:10 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO 02-17 00:38:14 [loggers.py:257] Engine 000: Avg prompt throughput: 265.2 tokens/s, Avg generation throughput: 393.8 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.5%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) INFO:     127.0.0.1:43120 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) INFO:     127.0.0.1:54422 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:38:17 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) WARNING 02-17 00:38:18 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:43112 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:38:20 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:39792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:38:23 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO 02-17 00:38:24 [loggers.py:257] Engine 000: Avg prompt throughput: 612.1 tokens/s, Avg generation throughput: 332.7 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.9%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) INFO:     127.0.0.1:33772 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:38:29 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:39320 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:38:33 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO 02-17 00:38:34 [loggers.py:257] Engine 000: Avg prompt throughput: 156.0 tokens/s, Avg generation throughput: 408.9 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.8%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) INFO:     127.0.0.1:39328 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:38:36 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:52810 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:38:41 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:39802 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) INFO:     127.0.0.1:52818 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:38:44 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO 02-17 00:38:44 [loggers.py:257] Engine 000: Avg prompt throughput: 1005.3 tokens/s, Avg generation throughput: 275.9 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.0%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) WARNING 02-17 00:38:45 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:52814 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:38:49 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:52176 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:38:52 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO 02-17 00:38:54 [loggers.py:257] Engine 000: Avg prompt throughput: 644.0 tokens/s, Avg generation throughput: 371.1 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.0%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) INFO:     127.0.0.1:52188 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) INFO:     127.0.0.1:52164 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:38:57 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) WARNING 02-17 00:38:57 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:43052 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) INFO:     127.0.0.1:43084 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:39:02 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) ERROR 02-17 00:39:02 [serving.py:323] Error in preprocessing prompt inputs
(APIServer pid=2355576) ERROR 02-17 00:39:02 [serving.py:323] Traceback (most recent call last):
(APIServer pid=2355576) ERROR 02-17 00:39:02 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/chat_completion/serving.py", line 301, in render_chat_request
(APIServer pid=2355576) ERROR 02-17 00:39:02 [serving.py:323]     conversation, engine_prompts = await self._preprocess_chat(
(APIServer pid=2355576) ERROR 02-17 00:39:02 [serving.py:323]                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=2355576) ERROR 02-17 00:39:02 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1219, in _preprocess_chat
(APIServer pid=2355576) ERROR 02-17 00:39:02 [serving.py:323]     engine_prompt = await self._tokenize_prompt_input_async(
(APIServer pid=2355576) ERROR 02-17 00:39:02 [serving.py:323]                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=2355576) ERROR 02-17 00:39:02 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1106, in _tokenize_prompt_input_async
(APIServer pid=2355576) ERROR 02-17 00:39:02 [serving.py:323]     async for result in self._tokenize_prompt_inputs_async(
(APIServer pid=2355576) ERROR 02-17 00:39:02 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1127, in _tokenize_prompt_inputs_async
(APIServer pid=2355576) ERROR 02-17 00:39:02 [serving.py:323]     yield await self._normalize_prompt_text_to_input(
(APIServer pid=2355576) ERROR 02-17 00:39:02 [serving.py:323]           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=2355576) ERROR 02-17 00:39:02 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 987, in _normalize_prompt_text_to_input
(APIServer pid=2355576) ERROR 02-17 00:39:02 [serving.py:323]     return self._validate_input(request, input_ids, input_text)
(APIServer pid=2355576) ERROR 02-17 00:39:02 [serving.py:323]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=2355576) ERROR 02-17 00:39:02 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1084, in _validate_input
(APIServer pid=2355576) ERROR 02-17 00:39:02 [serving.py:323]     raise VLLMValidationError(
(APIServer pid=2355576) ERROR 02-17 00:39:02 [serving.py:323] vllm.exceptions.VLLMValidationError: 'max_tokens' or 'max_completion_tokens' is too large: 16384. This model's maximum context length is 32768 tokens and your request has 24743 input tokens (16384 > 32768 - 24743). (parameter=max_tokens, value=16384)
(APIServer pid=2355576) INFO:     127.0.0.1:42236 - "POST /v1/chat/completions HTTP/1.1" 400 Bad Request
(APIServer pid=2355576) WARNING 02-17 00:39:02 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) ERROR 02-17 00:39:02 [serving.py:323] Error in preprocessing prompt inputs
(APIServer pid=2355576) ERROR 02-17 00:39:02 [serving.py:323] Traceback (most recent call last):
(APIServer pid=2355576) ERROR 02-17 00:39:02 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/chat_completion/serving.py", line 301, in render_chat_request
(APIServer pid=2355576) ERROR 02-17 00:39:02 [serving.py:323]     conversation, engine_prompts = await self._preprocess_chat(
(APIServer pid=2355576) ERROR 02-17 00:39:02 [serving.py:323]                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=2355576) ERROR 02-17 00:39:02 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1219, in _preprocess_chat
(APIServer pid=2355576) ERROR 02-17 00:39:02 [serving.py:323]     engine_prompt = await self._tokenize_prompt_input_async(
(APIServer pid=2355576) ERROR 02-17 00:39:02 [serving.py:323]                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=2355576) ERROR 02-17 00:39:02 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1106, in _tokenize_prompt_input_async
(APIServer pid=2355576) ERROR 02-17 00:39:02 [serving.py:323]     async for result in self._tokenize_prompt_inputs_async(
(APIServer pid=2355576) ERROR 02-17 00:39:02 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1127, in _tokenize_prompt_inputs_async
(APIServer pid=2355576) ERROR 02-17 00:39:02 [serving.py:323]     yield await self._normalize_prompt_text_to_input(
(APIServer pid=2355576) ERROR 02-17 00:39:02 [serving.py:323]           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=2355576) ERROR 02-17 00:39:02 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 987, in _normalize_prompt_text_to_input
(APIServer pid=2355576) ERROR 02-17 00:39:02 [serving.py:323]     return self._validate_input(request, input_ids, input_text)
(APIServer pid=2355576) ERROR 02-17 00:39:02 [serving.py:323]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=2355576) ERROR 02-17 00:39:02 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1084, in _validate_input
(APIServer pid=2355576) ERROR 02-17 00:39:02 [serving.py:323]     raise VLLMValidationError(
(APIServer pid=2355576) ERROR 02-17 00:39:02 [serving.py:323] vllm.exceptions.VLLMValidationError: 'max_tokens' or 'max_completion_tokens' is too large: 16384. This model's maximum context length is 32768 tokens and your request has 24743 input tokens (16384 > 32768 - 24743). (parameter=max_tokens, value=16384)
(APIServer pid=2355576) INFO:     127.0.0.1:42250 - "POST /v1/chat/completions HTTP/1.1" 400 Bad Request
(APIServer pid=2355576) WARNING 02-17 00:39:02 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) ERROR 02-17 00:39:02 [serving.py:323] Error in preprocessing prompt inputs
(APIServer pid=2355576) ERROR 02-17 00:39:02 [serving.py:323] Traceback (most recent call last):
(APIServer pid=2355576) ERROR 02-17 00:39:02 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/chat_completion/serving.py", line 301, in render_chat_request
(APIServer pid=2355576) ERROR 02-17 00:39:02 [serving.py:323]     conversation, engine_prompts = await self._preprocess_chat(
(APIServer pid=2355576) ERROR 02-17 00:39:02 [serving.py:323]                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=2355576) ERROR 02-17 00:39:02 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1219, in _preprocess_chat
(APIServer pid=2355576) ERROR 02-17 00:39:02 [serving.py:323]     engine_prompt = await self._tokenize_prompt_input_async(
(APIServer pid=2355576) ERROR 02-17 00:39:02 [serving.py:323]                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=2355576) ERROR 02-17 00:39:02 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1106, in _tokenize_prompt_input_async
(APIServer pid=2355576) ERROR 02-17 00:39:02 [serving.py:323]     async for result in self._tokenize_prompt_inputs_async(
(APIServer pid=2355576) ERROR 02-17 00:39:02 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1127, in _tokenize_prompt_inputs_async
(APIServer pid=2355576) ERROR 02-17 00:39:02 [serving.py:323]     yield await self._normalize_prompt_text_to_input(
(APIServer pid=2355576) ERROR 02-17 00:39:02 [serving.py:323]           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=2355576) ERROR 02-17 00:39:02 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 987, in _normalize_prompt_text_to_input
(APIServer pid=2355576) ERROR 02-17 00:39:02 [serving.py:323]     return self._validate_input(request, input_ids, input_text)
(APIServer pid=2355576) ERROR 02-17 00:39:02 [serving.py:323]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=2355576) ERROR 02-17 00:39:02 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1084, in _validate_input
(APIServer pid=2355576) ERROR 02-17 00:39:02 [serving.py:323]     raise VLLMValidationError(
(APIServer pid=2355576) ERROR 02-17 00:39:02 [serving.py:323] vllm.exceptions.VLLMValidationError: 'max_tokens' or 'max_completion_tokens' is too large: 16384. This model's maximum context length is 32768 tokens and your request has 24743 input tokens (16384 > 32768 - 24743). (parameter=max_tokens, value=16384)
(APIServer pid=2355576) INFO:     127.0.0.1:42256 - "POST /v1/chat/completions HTTP/1.1" 400 Bad Request
(APIServer pid=2355576) WARNING 02-17 00:39:02 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) ERROR 02-17 00:39:02 [serving.py:323] Error in preprocessing prompt inputs
(APIServer pid=2355576) ERROR 02-17 00:39:02 [serving.py:323] Traceback (most recent call last):
(APIServer pid=2355576) ERROR 02-17 00:39:02 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/chat_completion/serving.py", line 301, in render_chat_request
(APIServer pid=2355576) ERROR 02-17 00:39:02 [serving.py:323]     conversation, engine_prompts = await self._preprocess_chat(
(APIServer pid=2355576) ERROR 02-17 00:39:02 [serving.py:323]                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=2355576) ERROR 02-17 00:39:02 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1219, in _preprocess_chat
(APIServer pid=2355576) ERROR 02-17 00:39:02 [serving.py:323]     engine_prompt = await self._tokenize_prompt_input_async(
(APIServer pid=2355576) ERROR 02-17 00:39:02 [serving.py:323]                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=2355576) ERROR 02-17 00:39:02 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1106, in _tokenize_prompt_input_async
(APIServer pid=2355576) ERROR 02-17 00:39:02 [serving.py:323]     async for result in self._tokenize_prompt_inputs_async(
(APIServer pid=2355576) ERROR 02-17 00:39:02 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1127, in _tokenize_prompt_inputs_async
(APIServer pid=2355576) ERROR 02-17 00:39:02 [serving.py:323]     yield await self._normalize_prompt_text_to_input(
(APIServer pid=2355576) ERROR 02-17 00:39:02 [serving.py:323]           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=2355576) ERROR 02-17 00:39:02 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 987, in _normalize_prompt_text_to_input
(APIServer pid=2355576) ERROR 02-17 00:39:02 [serving.py:323]     return self._validate_input(request, input_ids, input_text)
(APIServer pid=2355576) ERROR 02-17 00:39:02 [serving.py:323]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=2355576) ERROR 02-17 00:39:02 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1084, in _validate_input
(APIServer pid=2355576) ERROR 02-17 00:39:02 [serving.py:323]     raise VLLMValidationError(
(APIServer pid=2355576) ERROR 02-17 00:39:02 [serving.py:323] vllm.exceptions.VLLMValidationError: 'max_tokens' or 'max_completion_tokens' is too large: 16384. This model's maximum context length is 32768 tokens and your request has 24743 input tokens (16384 > 32768 - 24743). (parameter=max_tokens, value=16384)
(APIServer pid=2355576) INFO:     127.0.0.1:42258 - "POST /v1/chat/completions HTTP/1.1" 400 Bad Request
(APIServer pid=2355576) WARNING 02-17 00:39:02 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) ERROR 02-17 00:39:02 [serving.py:323] Error in preprocessing prompt inputs
(APIServer pid=2355576) ERROR 02-17 00:39:02 [serving.py:323] Traceback (most recent call last):
(APIServer pid=2355576) ERROR 02-17 00:39:02 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/chat_completion/serving.py", line 301, in render_chat_request
(APIServer pid=2355576) ERROR 02-17 00:39:02 [serving.py:323]     conversation, engine_prompts = await self._preprocess_chat(
(APIServer pid=2355576) ERROR 02-17 00:39:02 [serving.py:323]                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=2355576) ERROR 02-17 00:39:02 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1219, in _preprocess_chat
(APIServer pid=2355576) ERROR 02-17 00:39:02 [serving.py:323]     engine_prompt = await self._tokenize_prompt_input_async(
(APIServer pid=2355576) ERROR 02-17 00:39:02 [serving.py:323]                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=2355576) ERROR 02-17 00:39:02 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1106, in _tokenize_prompt_input_async
(APIServer pid=2355576) ERROR 02-17 00:39:02 [serving.py:323]     async for result in self._tokenize_prompt_inputs_async(
(APIServer pid=2355576) ERROR 02-17 00:39:02 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1127, in _tokenize_prompt_inputs_async
(APIServer pid=2355576) ERROR 02-17 00:39:02 [serving.py:323]     yield await self._normalize_prompt_text_to_input(
(APIServer pid=2355576) ERROR 02-17 00:39:02 [serving.py:323]           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=2355576) ERROR 02-17 00:39:02 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 987, in _normalize_prompt_text_to_input
(APIServer pid=2355576) ERROR 02-17 00:39:02 [serving.py:323]     return self._validate_input(request, input_ids, input_text)
(APIServer pid=2355576) ERROR 02-17 00:39:02 [serving.py:323]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=2355576) ERROR 02-17 00:39:02 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1084, in _validate_input
(APIServer pid=2355576) ERROR 02-17 00:39:02 [serving.py:323]     raise VLLMValidationError(
(APIServer pid=2355576) ERROR 02-17 00:39:02 [serving.py:323] vllm.exceptions.VLLMValidationError: 'max_tokens' or 'max_completion_tokens' is too large: 16384. This model's maximum context length is 32768 tokens and your request has 24743 input tokens (16384 > 32768 - 24743). (parameter=max_tokens, value=16384)
(APIServer pid=2355576) INFO:     127.0.0.1:42270 - "POST /v1/chat/completions HTTP/1.1" 400 Bad Request
(APIServer pid=2355576) WARNING 02-17 00:39:02 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) WARNING 02-17 00:39:03 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO 02-17 00:39:04 [loggers.py:257] Engine 000: Avg prompt throughput: 397.8 tokens/s, Avg generation throughput: 354.5 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.9%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) INFO:     127.0.0.1:42286 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) INFO:     127.0.0.1:39796 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:39:05 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) WARNING 02-17 00:39:06 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:42288 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) INFO:     127.0.0.1:42298 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:39:09 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) WARNING 02-17 00:39:09 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:42316 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:39:13 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO 02-17 00:39:14 [loggers.py:257] Engine 000: Avg prompt throughput: 936.2 tokens/s, Avg generation throughput: 283.7 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.9%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) INFO:     127.0.0.1:43068 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:39:14 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:42304 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:39:17 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:37224 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:39:23 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO 02-17 00:39:24 [loggers.py:257] Engine 000: Avg prompt throughput: 529.3 tokens/s, Avg generation throughput: 363.3 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.1%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) INFO:     127.0.0.1:46008 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:39:26 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:37222 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) INFO:     127.0.0.1:46010 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:39:32 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) WARNING 02-17 00:39:33 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:37206 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:39:33 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO 02-17 00:39:34 [loggers.py:257] Engine 000: Avg prompt throughput: 277.0 tokens/s, Avg generation throughput: 375.1 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.0%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) INFO:     127.0.0.1:38206 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:39:41 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:38212 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) INFO 02-17 00:39:44 [loggers.py:257] Engine 000: Avg prompt throughput: 334.4 tokens/s, Avg generation throughput: 410.1 tokens/s, Running: 3 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.0%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) WARNING 02-17 00:39:44 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:38226 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:39:47 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:34422 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:39:53 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO 02-17 00:39:54 [loggers.py:257] Engine 000: Avg prompt throughput: 162.6 tokens/s, Avg generation throughput: 390.5 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.3%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) INFO:     127.0.0.1:38220 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) INFO:     127.0.0.1:60112 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:39:57 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:34414 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:40:00 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) ERROR 02-17 00:40:00 [serving.py:323] Error in preprocessing prompt inputs
(APIServer pid=2355576) ERROR 02-17 00:40:00 [serving.py:323] Traceback (most recent call last):
(APIServer pid=2355576) ERROR 02-17 00:40:00 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/chat_completion/serving.py", line 301, in render_chat_request
(APIServer pid=2355576) ERROR 02-17 00:40:00 [serving.py:323]     conversation, engine_prompts = await self._preprocess_chat(
(APIServer pid=2355576) ERROR 02-17 00:40:00 [serving.py:323]                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=2355576) ERROR 02-17 00:40:00 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1219, in _preprocess_chat
(APIServer pid=2355576) ERROR 02-17 00:40:00 [serving.py:323]     engine_prompt = await self._tokenize_prompt_input_async(
(APIServer pid=2355576) ERROR 02-17 00:40:00 [serving.py:323]                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=2355576) ERROR 02-17 00:40:00 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1106, in _tokenize_prompt_input_async
(APIServer pid=2355576) ERROR 02-17 00:40:00 [serving.py:323]     async for result in self._tokenize_prompt_inputs_async(
(APIServer pid=2355576) ERROR 02-17 00:40:00 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1127, in _tokenize_prompt_inputs_async
(APIServer pid=2355576) ERROR 02-17 00:40:00 [serving.py:323]     yield await self._normalize_prompt_text_to_input(
(APIServer pid=2355576) ERROR 02-17 00:40:00 [serving.py:323]           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=2355576) ERROR 02-17 00:40:00 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 987, in _normalize_prompt_text_to_input
(APIServer pid=2355576) ERROR 02-17 00:40:00 [serving.py:323]     return self._validate_input(request, input_ids, input_text)
(APIServer pid=2355576) ERROR 02-17 00:40:00 [serving.py:323]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=2355576) ERROR 02-17 00:40:00 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1084, in _validate_input
(APIServer pid=2355576) ERROR 02-17 00:40:00 [serving.py:323]     raise VLLMValidationError(
(APIServer pid=2355576) ERROR 02-17 00:40:00 [serving.py:323] vllm.exceptions.VLLMValidationError: 'max_tokens' or 'max_completion_tokens' is too large: 16384. This model's maximum context length is 32768 tokens and your request has 19904 input tokens (16384 > 32768 - 19904). (parameter=max_tokens, value=16384)
(APIServer pid=2355576) INFO:     127.0.0.1:60138 - "POST /v1/chat/completions HTTP/1.1" 400 Bad Request
(APIServer pid=2355576) WARNING 02-17 00:40:00 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) ERROR 02-17 00:40:00 [serving.py:323] Error in preprocessing prompt inputs
(APIServer pid=2355576) ERROR 02-17 00:40:00 [serving.py:323] Traceback (most recent call last):
(APIServer pid=2355576) ERROR 02-17 00:40:00 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/chat_completion/serving.py", line 301, in render_chat_request
(APIServer pid=2355576) ERROR 02-17 00:40:00 [serving.py:323]     conversation, engine_prompts = await self._preprocess_chat(
(APIServer pid=2355576) ERROR 02-17 00:40:00 [serving.py:323]                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=2355576) ERROR 02-17 00:40:00 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1219, in _preprocess_chat
(APIServer pid=2355576) ERROR 02-17 00:40:00 [serving.py:323]     engine_prompt = await self._tokenize_prompt_input_async(
(APIServer pid=2355576) ERROR 02-17 00:40:00 [serving.py:323]                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=2355576) ERROR 02-17 00:40:00 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1106, in _tokenize_prompt_input_async
(APIServer pid=2355576) ERROR 02-17 00:40:00 [serving.py:323]     async for result in self._tokenize_prompt_inputs_async(
(APIServer pid=2355576) ERROR 02-17 00:40:00 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1127, in _tokenize_prompt_inputs_async
(APIServer pid=2355576) ERROR 02-17 00:40:00 [serving.py:323]     yield await self._normalize_prompt_text_to_input(
(APIServer pid=2355576) ERROR 02-17 00:40:00 [serving.py:323]           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=2355576) ERROR 02-17 00:40:00 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 987, in _normalize_prompt_text_to_input
(APIServer pid=2355576) ERROR 02-17 00:40:00 [serving.py:323]     return self._validate_input(request, input_ids, input_text)
(APIServer pid=2355576) ERROR 02-17 00:40:00 [serving.py:323]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=2355576) ERROR 02-17 00:40:00 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1084, in _validate_input
(APIServer pid=2355576) ERROR 02-17 00:40:00 [serving.py:323]     raise VLLMValidationError(
(APIServer pid=2355576) ERROR 02-17 00:40:00 [serving.py:323] vllm.exceptions.VLLMValidationError: 'max_tokens' or 'max_completion_tokens' is too large: 16384. This model's maximum context length is 32768 tokens and your request has 19904 input tokens (16384 > 32768 - 19904). (parameter=max_tokens, value=16384)
(APIServer pid=2355576) INFO:     127.0.0.1:60140 - "POST /v1/chat/completions HTTP/1.1" 400 Bad Request
(APIServer pid=2355576) WARNING 02-17 00:40:00 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) WARNING 02-17 00:40:00 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) ERROR 02-17 00:40:00 [serving.py:323] Error in preprocessing prompt inputs
(APIServer pid=2355576) ERROR 02-17 00:40:00 [serving.py:323] Traceback (most recent call last):
(APIServer pid=2355576) ERROR 02-17 00:40:00 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/chat_completion/serving.py", line 301, in render_chat_request
(APIServer pid=2355576) ERROR 02-17 00:40:00 [serving.py:323]     conversation, engine_prompts = await self._preprocess_chat(
(APIServer pid=2355576) ERROR 02-17 00:40:00 [serving.py:323]                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=2355576) ERROR 02-17 00:40:00 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1219, in _preprocess_chat
(APIServer pid=2355576) ERROR 02-17 00:40:00 [serving.py:323]     engine_prompt = await self._tokenize_prompt_input_async(
(APIServer pid=2355576) ERROR 02-17 00:40:00 [serving.py:323]                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=2355576) ERROR 02-17 00:40:00 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1106, in _tokenize_prompt_input_async
(APIServer pid=2355576) ERROR 02-17 00:40:00 [serving.py:323]     async for result in self._tokenize_prompt_inputs_async(
(APIServer pid=2355576) ERROR 02-17 00:40:00 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1127, in _tokenize_prompt_inputs_async
(APIServer pid=2355576) ERROR 02-17 00:40:00 [serving.py:323]     yield await self._normalize_prompt_text_to_input(
(APIServer pid=2355576) ERROR 02-17 00:40:00 [serving.py:323]           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=2355576) ERROR 02-17 00:40:00 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 987, in _normalize_prompt_text_to_input
(APIServer pid=2355576) ERROR 02-17 00:40:00 [serving.py:323]     return self._validate_input(request, input_ids, input_text)
(APIServer pid=2355576) ERROR 02-17 00:40:00 [serving.py:323]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=2355576) ERROR 02-17 00:40:00 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1084, in _validate_input
(APIServer pid=2355576) ERROR 02-17 00:40:00 [serving.py:323]     raise VLLMValidationError(
(APIServer pid=2355576) ERROR 02-17 00:40:00 [serving.py:323] vllm.exceptions.VLLMValidationError: 'max_tokens' or 'max_completion_tokens' is too large: 16384. This model's maximum context length is 32768 tokens and your request has 19904 input tokens (16384 > 32768 - 19904). (parameter=max_tokens, value=16384)
(APIServer pid=2355576) INFO:     127.0.0.1:60156 - "POST /v1/chat/completions HTTP/1.1" 400 Bad Request
(APIServer pid=2355576) WARNING 02-17 00:40:00 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) ERROR 02-17 00:40:00 [serving.py:323] Error in preprocessing prompt inputs
(APIServer pid=2355576) ERROR 02-17 00:40:00 [serving.py:323] Traceback (most recent call last):
(APIServer pid=2355576) ERROR 02-17 00:40:00 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/chat_completion/serving.py", line 301, in render_chat_request
(APIServer pid=2355576) ERROR 02-17 00:40:00 [serving.py:323]     conversation, engine_prompts = await self._preprocess_chat(
(APIServer pid=2355576) ERROR 02-17 00:40:00 [serving.py:323]                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=2355576) ERROR 02-17 00:40:00 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1219, in _preprocess_chat
(APIServer pid=2355576) ERROR 02-17 00:40:00 [serving.py:323]     engine_prompt = await self._tokenize_prompt_input_async(
(APIServer pid=2355576) ERROR 02-17 00:40:00 [serving.py:323]                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=2355576) ERROR 02-17 00:40:00 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1106, in _tokenize_prompt_input_async
(APIServer pid=2355576) ERROR 02-17 00:40:00 [serving.py:323]     async for result in self._tokenize_prompt_inputs_async(
(APIServer pid=2355576) ERROR 02-17 00:40:00 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1127, in _tokenize_prompt_inputs_async
(APIServer pid=2355576) ERROR 02-17 00:40:00 [serving.py:323]     yield await self._normalize_prompt_text_to_input(
(APIServer pid=2355576) ERROR 02-17 00:40:00 [serving.py:323]           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=2355576) ERROR 02-17 00:40:00 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 987, in _normalize_prompt_text_to_input
(APIServer pid=2355576) ERROR 02-17 00:40:00 [serving.py:323]     return self._validate_input(request, input_ids, input_text)
(APIServer pid=2355576) ERROR 02-17 00:40:00 [serving.py:323]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=2355576) ERROR 02-17 00:40:00 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1084, in _validate_input
(APIServer pid=2355576) ERROR 02-17 00:40:00 [serving.py:323]     raise VLLMValidationError(
(APIServer pid=2355576) ERROR 02-17 00:40:00 [serving.py:323] vllm.exceptions.VLLMValidationError: 'max_tokens' or 'max_completion_tokens' is too large: 16384. This model's maximum context length is 32768 tokens and your request has 19904 input tokens (16384 > 32768 - 19904). (parameter=max_tokens, value=16384)
(APIServer pid=2355576) INFO:     127.0.0.1:60174 - "POST /v1/chat/completions HTTP/1.1" 400 Bad Request
(APIServer pid=2355576) WARNING 02-17 00:40:00 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) ERROR 02-17 00:40:00 [serving.py:323] Error in preprocessing prompt inputs
(APIServer pid=2355576) ERROR 02-17 00:40:00 [serving.py:323] Traceback (most recent call last):
(APIServer pid=2355576) ERROR 02-17 00:40:00 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/chat_completion/serving.py", line 301, in render_chat_request
(APIServer pid=2355576) ERROR 02-17 00:40:00 [serving.py:323]     conversation, engine_prompts = await self._preprocess_chat(
(APIServer pid=2355576) ERROR 02-17 00:40:00 [serving.py:323]                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=2355576) ERROR 02-17 00:40:00 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1219, in _preprocess_chat
(APIServer pid=2355576) ERROR 02-17 00:40:00 [serving.py:323]     engine_prompt = await self._tokenize_prompt_input_async(
(APIServer pid=2355576) ERROR 02-17 00:40:00 [serving.py:323]                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=2355576) ERROR 02-17 00:40:00 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1106, in _tokenize_prompt_input_async
(APIServer pid=2355576) ERROR 02-17 00:40:00 [serving.py:323]     async for result in self._tokenize_prompt_inputs_async(
(APIServer pid=2355576) ERROR 02-17 00:40:00 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1127, in _tokenize_prompt_inputs_async
(APIServer pid=2355576) ERROR 02-17 00:40:00 [serving.py:323]     yield await self._normalize_prompt_text_to_input(
(APIServer pid=2355576) ERROR 02-17 00:40:00 [serving.py:323]           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=2355576) ERROR 02-17 00:40:00 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 987, in _normalize_prompt_text_to_input
(APIServer pid=2355576) ERROR 02-17 00:40:00 [serving.py:323]     return self._validate_input(request, input_ids, input_text)
(APIServer pid=2355576) ERROR 02-17 00:40:00 [serving.py:323]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=2355576) ERROR 02-17 00:40:00 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1084, in _validate_input
(APIServer pid=2355576) ERROR 02-17 00:40:00 [serving.py:323]     raise VLLMValidationError(
(APIServer pid=2355576) ERROR 02-17 00:40:00 [serving.py:323] vllm.exceptions.VLLMValidationError: 'max_tokens' or 'max_completion_tokens' is too large: 16384. This model's maximum context length is 32768 tokens and your request has 19904 input tokens (16384 > 32768 - 19904). (parameter=max_tokens, value=16384)
(APIServer pid=2355576) INFO:     127.0.0.1:60182 - "POST /v1/chat/completions HTTP/1.1" 400 Bad Request
(APIServer pid=2355576) WARNING 02-17 00:40:00 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:60124 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) INFO:     127.0.0.1:60192 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) INFO 02-17 00:40:04 [loggers.py:257] Engine 000: Avg prompt throughput: 659.0 tokens/s, Avg generation throughput: 281.2 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.9%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) WARNING 02-17 00:40:04 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) WARNING 02-17 00:40:04 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:42294 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:40:09 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:60168 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:40:13 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO 02-17 00:40:14 [loggers.py:257] Engine 000: Avg prompt throughput: 635.4 tokens/s, Avg generation throughput: 358.0 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.8%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) INFO 02-17 00:40:24 [loggers.py:257] Engine 000: Avg prompt throughput: 204.3 tokens/s, Avg generation throughput: 259.7 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.0%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) INFO:     127.0.0.1:46872 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:40:29 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO 02-17 00:40:34 [loggers.py:257] Engine 000: Avg prompt throughput: 98.6 tokens/s, Avg generation throughput: 437.8 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.1%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) INFO:     127.0.0.1:42098 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:40:36 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:53054 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:40:38 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:36388 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:40:41 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO 02-17 00:40:44 [loggers.py:257] Engine 000: Avg prompt throughput: 361.7 tokens/s, Avg generation throughput: 394.9 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.1%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) INFO:     127.0.0.1:36406 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:40:47 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:46892 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:40:50 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO 02-17 00:40:54 [loggers.py:257] Engine 000: Avg prompt throughput: 506.2 tokens/s, Avg generation throughput: 382.4 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.2%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) INFO:     127.0.0.1:36396 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) INFO:     127.0.0.1:46882 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:40:55 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) WARNING 02-17 00:40:56 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:35788 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:40:59 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO 02-17 00:41:04 [loggers.py:257] Engine 000: Avg prompt throughput: 933.0 tokens/s, Avg generation throughput: 336.0 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.2%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) INFO:     127.0.0.1:51358 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:41:13 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO 02-17 00:41:14 [loggers.py:257] Engine 000: Avg prompt throughput: 75.5 tokens/s, Avg generation throughput: 435.0 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.3%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) INFO:     127.0.0.1:53664 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:41:20 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO 02-17 00:41:24 [loggers.py:257] Engine 000: Avg prompt throughput: 73.1 tokens/s, Avg generation throughput: 433.0 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.6%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) INFO:     127.0.0.1:53676 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:41:26 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:58516 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:41:31 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO 02-17 00:41:34 [loggers.py:257] Engine 000: Avg prompt throughput: 157.8 tokens/s, Avg generation throughput: 400.2 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.8%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) INFO:     127.0.0.1:35792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) INFO:     127.0.0.1:58522 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) INFO:     127.0.0.1:51350 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:41:38 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) WARNING 02-17 00:41:38 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) WARNING 02-17 00:41:40 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO 02-17 00:41:44 [loggers.py:257] Engine 000: Avg prompt throughput: 955.5 tokens/s, Avg generation throughput: 310.6 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.5%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) INFO 02-17 00:41:54 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 459.6 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.8%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) INFO:     127.0.0.1:38508 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:41:55 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:51342 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) INFO 02-17 00:42:04 [loggers.py:257] Engine 000: Avg prompt throughput: 225.4 tokens/s, Avg generation throughput: 419.8 tokens/s, Running: 3 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.2%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) INFO:     127.0.0.1:38510 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:42:04 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) WARNING 02-17 00:42:05 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:36558 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:42:10 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO 02-17 00:42:14 [loggers.py:257] Engine 000: Avg prompt throughput: 669.3 tokens/s, Avg generation throughput: 347.4 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.4%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) INFO:     127.0.0.1:36578 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:42:15 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:56470 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:42:18 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO 02-17 00:42:24 [loggers.py:257] Engine 000: Avg prompt throughput: 244.0 tokens/s, Avg generation throughput: 398.1 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.5%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) INFO:     127.0.0.1:45824 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:42:29 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:38494 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) INFO:     127.0.0.1:36562 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:42:30 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) WARNING 02-17 00:42:31 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO 02-17 00:42:34 [loggers.py:257] Engine 000: Avg prompt throughput: 570.0 tokens/s, Avg generation throughput: 365.7 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.8%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) INFO:     127.0.0.1:35458 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:42:39 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(Worker_TP3 pid=2356195) /users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/model_executor/layers/fla/ops/utils.py:113: UserWarning: Input tensor shape suggests potential format mismatch: seq_len (6) < num_heads (8). This may indicate the inputs were passed in head-first format [B, H, T, ...] when head_first=False was specified. Please verify your input tensor format matches the expected shape [B, T, H, ...].
(Worker_TP0 pid=2356192) /users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/model_executor/layers/fla/ops/utils.py:113: UserWarning: Input tensor shape suggests potential format mismatch: seq_len (6) < num_heads (8). This may indicate the inputs were passed in head-first format [B, H, T, ...] when head_first=False was specified. Please verify your input tensor format matches the expected shape [B, T, H, ...].
(Worker_TP3 pid=2356195)   return fn(*contiguous_args, **contiguous_kwargs)
(Worker_TP1 pid=2356193) /users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/model_executor/layers/fla/ops/utils.py:113: UserWarning: Input tensor shape suggests potential format mismatch: seq_len (6) < num_heads (8). This may indicate the inputs were passed in head-first format [B, H, T, ...] when head_first=False was specified. Please verify your input tensor format matches the expected shape [B, T, H, ...].
(Worker_TP0 pid=2356192)   return fn(*contiguous_args, **contiguous_kwargs)
(Worker_TP1 pid=2356193)   return fn(*contiguous_args, **contiguous_kwargs)
(Worker_TP2 pid=2356194) /users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/model_executor/layers/fla/ops/utils.py:113: UserWarning: Input tensor shape suggests potential format mismatch: seq_len (6) < num_heads (8). This may indicate the inputs were passed in head-first format [B, H, T, ...] when head_first=False was specified. Please verify your input tensor format matches the expected shape [B, T, H, ...].
(Worker_TP2 pid=2356194)   return fn(*contiguous_args, **contiguous_kwargs)
(APIServer pid=2355576) INFO 02-17 00:42:44 [loggers.py:257] Engine 000: Avg prompt throughput: 153.0 tokens/s, Avg generation throughput: 452.1 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.1%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) INFO:     127.0.0.1:45840 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:42:46 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:60312 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:42:51 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO 02-17 00:42:54 [loggers.py:257] Engine 000: Avg prompt throughput: 497.5 tokens/s, Avg generation throughput: 389.2 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.2%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) INFO:     127.0.0.1:35460 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:43:01 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO 02-17 00:43:04 [loggers.py:257] Engine 000: Avg prompt throughput: 751.9 tokens/s, Avg generation throughput: 359.7 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.4%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) INFO 02-17 00:43:14 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 463.2 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.8%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) INFO:     127.0.0.1:35464 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:43:18 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:36372 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) INFO 02-17 00:43:24 [loggers.py:257] Engine 000: Avg prompt throughput: 525.0 tokens/s, Avg generation throughput: 385.4 tokens/s, Running: 3 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.4%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) INFO:     127.0.0.1:55866 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:43:24 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) WARNING 02-17 00:43:24 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO 02-17 00:43:34 [loggers.py:257] Engine 000: Avg prompt throughput: 450.2 tokens/s, Avg generation throughput: 406.9 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.4%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) INFO 02-17 00:43:44 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 464.0 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.7%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) INFO:     127.0.0.1:51182 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:43:48 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:51176 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:43:52 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO 02-17 00:43:54 [loggers.py:257] Engine 000: Avg prompt throughput: 734.1 tokens/s, Avg generation throughput: 352.1 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.7%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) INFO 02-17 00:44:04 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 458.4 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.0%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) INFO 02-17 00:44:14 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 456.0 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.3%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) INFO:     127.0.0.1:48112 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:44:16 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:48174 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:44:18 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO 02-17 00:44:24 [loggers.py:257] Engine 000: Avg prompt throughput: 379.0 tokens/s, Avg generation throughput: 377.3 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.7%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) INFO 02-17 00:44:34 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 460.0 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.0%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) INFO 02-17 00:44:44 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 457.6 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.2%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) INFO:     127.0.0.1:54478 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:44:49 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO 02-17 00:44:54 [loggers.py:257] Engine 000: Avg prompt throughput: 412.6 tokens/s, Avg generation throughput: 401.0 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.4%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) INFO:     127.0.0.1:36386 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:44:59 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO 02-17 00:45:04 [loggers.py:257] Engine 000: Avg prompt throughput: 1434.2 tokens/s, Avg generation throughput: 281.9 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.5%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) INFO:     127.0.0.1:54470 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:45:05 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO 02-17 00:45:14 [loggers.py:257] Engine 000: Avg prompt throughput: 532.2 tokens/s, Avg generation throughput: 384.7 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.7%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) INFO 02-17 00:45:24 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 446.4 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.9%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) INFO 02-17 00:45:34 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 443.6 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 3.2%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) INFO:     127.0.0.1:47162 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:45:36 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:38476 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:45:43 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO 02-17 00:45:44 [loggers.py:257] Engine 000: Avg prompt throughput: 1333.7 tokens/s, Avg generation throughput: 271.6 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.7%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) INFO 02-17 00:45:54 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 445.2 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 3.0%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) INFO:     127.0.0.1:42784 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) INFO 02-17 00:46:04 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 441.1 tokens/s, Running: 3 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.5%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) WARNING 02-17 00:46:05 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:37764 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:46:05 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) ERROR 02-17 00:46:05 [serving.py:323] Error in preprocessing prompt inputs
(APIServer pid=2355576) ERROR 02-17 00:46:05 [serving.py:323] Traceback (most recent call last):
(APIServer pid=2355576) ERROR 02-17 00:46:05 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/chat_completion/serving.py", line 301, in render_chat_request
(APIServer pid=2355576) ERROR 02-17 00:46:05 [serving.py:323]     conversation, engine_prompts = await self._preprocess_chat(
(APIServer pid=2355576) ERROR 02-17 00:46:05 [serving.py:323]                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=2355576) ERROR 02-17 00:46:05 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1219, in _preprocess_chat
(APIServer pid=2355576) ERROR 02-17 00:46:05 [serving.py:323]     engine_prompt = await self._tokenize_prompt_input_async(
(APIServer pid=2355576) ERROR 02-17 00:46:05 [serving.py:323]                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=2355576) ERROR 02-17 00:46:05 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1106, in _tokenize_prompt_input_async
(APIServer pid=2355576) ERROR 02-17 00:46:05 [serving.py:323]     async for result in self._tokenize_prompt_inputs_async(
(APIServer pid=2355576) ERROR 02-17 00:46:05 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1127, in _tokenize_prompt_inputs_async
(APIServer pid=2355576) ERROR 02-17 00:46:05 [serving.py:323]     yield await self._normalize_prompt_text_to_input(
(APIServer pid=2355576) ERROR 02-17 00:46:05 [serving.py:323]           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=2355576) ERROR 02-17 00:46:05 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 987, in _normalize_prompt_text_to_input
(APIServer pid=2355576) ERROR 02-17 00:46:05 [serving.py:323]     return self._validate_input(request, input_ids, input_text)
(APIServer pid=2355576) ERROR 02-17 00:46:05 [serving.py:323]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=2355576) ERROR 02-17 00:46:05 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1084, in _validate_input
(APIServer pid=2355576) ERROR 02-17 00:46:05 [serving.py:323]     raise VLLMValidationError(
(APIServer pid=2355576) ERROR 02-17 00:46:05 [serving.py:323] vllm.exceptions.VLLMValidationError: 'max_tokens' or 'max_completion_tokens' is too large: 16384. This model's maximum context length is 32768 tokens and your request has 18127 input tokens (16384 > 32768 - 18127). (parameter=max_tokens, value=16384)
(APIServer pid=2355576) INFO:     127.0.0.1:60688 - "POST /v1/chat/completions HTTP/1.1" 400 Bad Request
(APIServer pid=2355576) WARNING 02-17 00:46:06 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) ERROR 02-17 00:46:06 [serving.py:323] Error in preprocessing prompt inputs
(APIServer pid=2355576) ERROR 02-17 00:46:06 [serving.py:323] Traceback (most recent call last):
(APIServer pid=2355576) ERROR 02-17 00:46:06 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/chat_completion/serving.py", line 301, in render_chat_request
(APIServer pid=2355576) ERROR 02-17 00:46:06 [serving.py:323]     conversation, engine_prompts = await self._preprocess_chat(
(APIServer pid=2355576) ERROR 02-17 00:46:06 [serving.py:323]                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=2355576) ERROR 02-17 00:46:06 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1219, in _preprocess_chat
(APIServer pid=2355576) ERROR 02-17 00:46:06 [serving.py:323]     engine_prompt = await self._tokenize_prompt_input_async(
(APIServer pid=2355576) ERROR 02-17 00:46:06 [serving.py:323]                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=2355576) ERROR 02-17 00:46:06 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1106, in _tokenize_prompt_input_async
(APIServer pid=2355576) ERROR 02-17 00:46:06 [serving.py:323]     async for result in self._tokenize_prompt_inputs_async(
(APIServer pid=2355576) ERROR 02-17 00:46:06 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1127, in _tokenize_prompt_inputs_async
(APIServer pid=2355576) ERROR 02-17 00:46:06 [serving.py:323]     yield await self._normalize_prompt_text_to_input(
(APIServer pid=2355576) ERROR 02-17 00:46:06 [serving.py:323]           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=2355576) ERROR 02-17 00:46:06 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 987, in _normalize_prompt_text_to_input
(APIServer pid=2355576) ERROR 02-17 00:46:06 [serving.py:323]     return self._validate_input(request, input_ids, input_text)
(APIServer pid=2355576) ERROR 02-17 00:46:06 [serving.py:323]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=2355576) ERROR 02-17 00:46:06 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1084, in _validate_input
(APIServer pid=2355576) ERROR 02-17 00:46:06 [serving.py:323]     raise VLLMValidationError(
(APIServer pid=2355576) ERROR 02-17 00:46:06 [serving.py:323] vllm.exceptions.VLLMValidationError: 'max_tokens' or 'max_completion_tokens' is too large: 16384. This model's maximum context length is 32768 tokens and your request has 18127 input tokens (16384 > 32768 - 18127). (parameter=max_tokens, value=16384)
(APIServer pid=2355576) INFO:     127.0.0.1:60704 - "POST /v1/chat/completions HTTP/1.1" 400 Bad Request
(APIServer pid=2355576) WARNING 02-17 00:46:06 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) ERROR 02-17 00:46:06 [serving.py:323] Error in preprocessing prompt inputs
(APIServer pid=2355576) ERROR 02-17 00:46:06 [serving.py:323] Traceback (most recent call last):
(APIServer pid=2355576) ERROR 02-17 00:46:06 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/chat_completion/serving.py", line 301, in render_chat_request
(APIServer pid=2355576) ERROR 02-17 00:46:06 [serving.py:323]     conversation, engine_prompts = await self._preprocess_chat(
(APIServer pid=2355576) ERROR 02-17 00:46:06 [serving.py:323]                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=2355576) ERROR 02-17 00:46:06 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1219, in _preprocess_chat
(APIServer pid=2355576) ERROR 02-17 00:46:06 [serving.py:323]     engine_prompt = await self._tokenize_prompt_input_async(
(APIServer pid=2355576) ERROR 02-17 00:46:06 [serving.py:323]                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=2355576) ERROR 02-17 00:46:06 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1106, in _tokenize_prompt_input_async
(APIServer pid=2355576) ERROR 02-17 00:46:06 [serving.py:323]     async for result in self._tokenize_prompt_inputs_async(
(APIServer pid=2355576) ERROR 02-17 00:46:06 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1127, in _tokenize_prompt_inputs_async
(APIServer pid=2355576) ERROR 02-17 00:46:06 [serving.py:323]     yield await self._normalize_prompt_text_to_input(
(APIServer pid=2355576) ERROR 02-17 00:46:06 [serving.py:323]           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=2355576) ERROR 02-17 00:46:06 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 987, in _normalize_prompt_text_to_input
(APIServer pid=2355576) ERROR 02-17 00:46:06 [serving.py:323]     return self._validate_input(request, input_ids, input_text)
(APIServer pid=2355576) ERROR 02-17 00:46:06 [serving.py:323]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=2355576) ERROR 02-17 00:46:06 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1084, in _validate_input
(APIServer pid=2355576) ERROR 02-17 00:46:06 [serving.py:323]     raise VLLMValidationError(
(APIServer pid=2355576) ERROR 02-17 00:46:06 [serving.py:323] vllm.exceptions.VLLMValidationError: 'max_tokens' or 'max_completion_tokens' is too large: 16384. This model's maximum context length is 32768 tokens and your request has 18127 input tokens (16384 > 32768 - 18127). (parameter=max_tokens, value=16384)
(APIServer pid=2355576) INFO:     127.0.0.1:60714 - "POST /v1/chat/completions HTTP/1.1" 400 Bad Request
(APIServer pid=2355576) WARNING 02-17 00:46:06 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) ERROR 02-17 00:46:06 [serving.py:323] Error in preprocessing prompt inputs
(APIServer pid=2355576) ERROR 02-17 00:46:06 [serving.py:323] Traceback (most recent call last):
(APIServer pid=2355576) ERROR 02-17 00:46:06 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/chat_completion/serving.py", line 301, in render_chat_request
(APIServer pid=2355576) ERROR 02-17 00:46:06 [serving.py:323]     conversation, engine_prompts = await self._preprocess_chat(
(APIServer pid=2355576) ERROR 02-17 00:46:06 [serving.py:323]                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=2355576) ERROR 02-17 00:46:06 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1219, in _preprocess_chat
(APIServer pid=2355576) ERROR 02-17 00:46:06 [serving.py:323]     engine_prompt = await self._tokenize_prompt_input_async(
(APIServer pid=2355576) ERROR 02-17 00:46:06 [serving.py:323]                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=2355576) ERROR 02-17 00:46:06 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1106, in _tokenize_prompt_input_async
(APIServer pid=2355576) ERROR 02-17 00:46:06 [serving.py:323]     async for result in self._tokenize_prompt_inputs_async(
(APIServer pid=2355576) ERROR 02-17 00:46:06 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1127, in _tokenize_prompt_inputs_async
(APIServer pid=2355576) ERROR 02-17 00:46:06 [serving.py:323]     yield await self._normalize_prompt_text_to_input(
(APIServer pid=2355576) ERROR 02-17 00:46:06 [serving.py:323]           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=2355576) ERROR 02-17 00:46:06 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 987, in _normalize_prompt_text_to_input
(APIServer pid=2355576) ERROR 02-17 00:46:06 [serving.py:323]     return self._validate_input(request, input_ids, input_text)
(APIServer pid=2355576) ERROR 02-17 00:46:06 [serving.py:323]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=2355576) ERROR 02-17 00:46:06 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1084, in _validate_input
(APIServer pid=2355576) ERROR 02-17 00:46:06 [serving.py:323]     raise VLLMValidationError(
(APIServer pid=2355576) ERROR 02-17 00:46:06 [serving.py:323] vllm.exceptions.VLLMValidationError: 'max_tokens' or 'max_completion_tokens' is too large: 16384. This model's maximum context length is 32768 tokens and your request has 18127 input tokens (16384 > 32768 - 18127). (parameter=max_tokens, value=16384)
(APIServer pid=2355576) INFO:     127.0.0.1:60722 - "POST /v1/chat/completions HTTP/1.1" 400 Bad Request
(APIServer pid=2355576) WARNING 02-17 00:46:06 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) ERROR 02-17 00:46:06 [serving.py:323] Error in preprocessing prompt inputs
(APIServer pid=2355576) ERROR 02-17 00:46:06 [serving.py:323] Traceback (most recent call last):
(APIServer pid=2355576) ERROR 02-17 00:46:06 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/chat_completion/serving.py", line 301, in render_chat_request
(APIServer pid=2355576) ERROR 02-17 00:46:06 [serving.py:323]     conversation, engine_prompts = await self._preprocess_chat(
(APIServer pid=2355576) ERROR 02-17 00:46:06 [serving.py:323]                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=2355576) ERROR 02-17 00:46:06 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1219, in _preprocess_chat
(APIServer pid=2355576) ERROR 02-17 00:46:06 [serving.py:323]     engine_prompt = await self._tokenize_prompt_input_async(
(APIServer pid=2355576) ERROR 02-17 00:46:06 [serving.py:323]                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=2355576) ERROR 02-17 00:46:06 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1106, in _tokenize_prompt_input_async
(APIServer pid=2355576) ERROR 02-17 00:46:06 [serving.py:323]     async for result in self._tokenize_prompt_inputs_async(
(APIServer pid=2355576) ERROR 02-17 00:46:06 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1127, in _tokenize_prompt_inputs_async
(APIServer pid=2355576) ERROR 02-17 00:46:06 [serving.py:323]     yield await self._normalize_prompt_text_to_input(
(APIServer pid=2355576) ERROR 02-17 00:46:06 [serving.py:323]           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=2355576) ERROR 02-17 00:46:06 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 987, in _normalize_prompt_text_to_input
(APIServer pid=2355576) ERROR 02-17 00:46:06 [serving.py:323]     return self._validate_input(request, input_ids, input_text)
(APIServer pid=2355576) ERROR 02-17 00:46:06 [serving.py:323]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=2355576) ERROR 02-17 00:46:06 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1084, in _validate_input
(APIServer pid=2355576) ERROR 02-17 00:46:06 [serving.py:323]     raise VLLMValidationError(
(APIServer pid=2355576) ERROR 02-17 00:46:06 [serving.py:323] vllm.exceptions.VLLMValidationError: 'max_tokens' or 'max_completion_tokens' is too large: 16384. This model's maximum context length is 32768 tokens and your request has 18127 input tokens (16384 > 32768 - 18127). (parameter=max_tokens, value=16384)
(APIServer pid=2355576) INFO:     127.0.0.1:60732 - "POST /v1/chat/completions HTTP/1.1" 400 Bad Request
(APIServer pid=2355576) WARNING 02-17 00:46:06 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:60744 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:46:09 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO 02-17 00:46:14 [loggers.py:257] Engine 000: Avg prompt throughput: 606.6 tokens/s, Avg generation throughput: 334.2 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.8%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) INFO:     127.0.0.1:60750 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:46:23 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO 02-17 00:46:24 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 419.2 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.9%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) INFO:     127.0.0.1:38472 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:46:30 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO 02-17 00:46:34 [loggers.py:257] Engine 000: Avg prompt throughput: 313.8 tokens/s, Avg generation throughput: 392.4 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 3.1%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) INFO:     127.0.0.1:60690 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:46:39 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:38480 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) INFO 02-17 00:46:44 [loggers.py:257] Engine 000: Avg prompt throughput: 366.2 tokens/s, Avg generation throughput: 372.8 tokens/s, Running: 3 reqs, Waiting: 0 reqs, GPU KV cache usage: 3.0%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) WARNING 02-17 00:46:45 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:49948 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) INFO:     127.0.0.1:44730 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:46:49 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) WARNING 02-17 00:46:50 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) ERROR 02-17 00:46:50 [serving.py:323] Error in preprocessing prompt inputs
(APIServer pid=2355576) ERROR 02-17 00:46:50 [serving.py:323] Traceback (most recent call last):
(APIServer pid=2355576) ERROR 02-17 00:46:50 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/chat_completion/serving.py", line 301, in render_chat_request
(APIServer pid=2355576) ERROR 02-17 00:46:50 [serving.py:323]     conversation, engine_prompts = await self._preprocess_chat(
(APIServer pid=2355576) ERROR 02-17 00:46:50 [serving.py:323]                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=2355576) ERROR 02-17 00:46:50 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1219, in _preprocess_chat
(APIServer pid=2355576) ERROR 02-17 00:46:50 [serving.py:323]     engine_prompt = await self._tokenize_prompt_input_async(
(APIServer pid=2355576) ERROR 02-17 00:46:50 [serving.py:323]                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=2355576) ERROR 02-17 00:46:50 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1106, in _tokenize_prompt_input_async
(APIServer pid=2355576) ERROR 02-17 00:46:50 [serving.py:323]     async for result in self._tokenize_prompt_inputs_async(
(APIServer pid=2355576) ERROR 02-17 00:46:50 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1127, in _tokenize_prompt_inputs_async
(APIServer pid=2355576) ERROR 02-17 00:46:50 [serving.py:323]     yield await self._normalize_prompt_text_to_input(
(APIServer pid=2355576) ERROR 02-17 00:46:50 [serving.py:323]           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=2355576) ERROR 02-17 00:46:50 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 987, in _normalize_prompt_text_to_input
(APIServer pid=2355576) ERROR 02-17 00:46:50 [serving.py:323]     return self._validate_input(request, input_ids, input_text)
(APIServer pid=2355576) ERROR 02-17 00:46:50 [serving.py:323]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=2355576) ERROR 02-17 00:46:50 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1074, in _validate_input
(APIServer pid=2355576) ERROR 02-17 00:46:50 [serving.py:323]     raise VLLMValidationError(
(APIServer pid=2355576) ERROR 02-17 00:46:50 [serving.py:323] vllm.exceptions.VLLMValidationError: This model's maximum context length is 32768 tokens. However, your request has 33119 input tokens. Please reduce the length of the input messages. (parameter=input_tokens, value=33119)
(APIServer pid=2355576) INFO:     127.0.0.1:49958 - "POST /v1/chat/completions HTTP/1.1" 400 Bad Request
(APIServer pid=2355576) WARNING 02-17 00:46:50 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) ERROR 02-17 00:46:50 [serving.py:323] Error in preprocessing prompt inputs
(APIServer pid=2355576) ERROR 02-17 00:46:50 [serving.py:323] Traceback (most recent call last):
(APIServer pid=2355576) ERROR 02-17 00:46:50 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/chat_completion/serving.py", line 301, in render_chat_request
(APIServer pid=2355576) ERROR 02-17 00:46:50 [serving.py:323]     conversation, engine_prompts = await self._preprocess_chat(
(APIServer pid=2355576) ERROR 02-17 00:46:50 [serving.py:323]                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=2355576) ERROR 02-17 00:46:50 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1219, in _preprocess_chat
(APIServer pid=2355576) ERROR 02-17 00:46:50 [serving.py:323]     engine_prompt = await self._tokenize_prompt_input_async(
(APIServer pid=2355576) ERROR 02-17 00:46:50 [serving.py:323]                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=2355576) ERROR 02-17 00:46:50 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1106, in _tokenize_prompt_input_async
(APIServer pid=2355576) ERROR 02-17 00:46:50 [serving.py:323]     async for result in self._tokenize_prompt_inputs_async(
(APIServer pid=2355576) ERROR 02-17 00:46:50 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1127, in _tokenize_prompt_inputs_async
(APIServer pid=2355576) ERROR 02-17 00:46:50 [serving.py:323]     yield await self._normalize_prompt_text_to_input(
(APIServer pid=2355576) ERROR 02-17 00:46:50 [serving.py:323]           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=2355576) ERROR 02-17 00:46:50 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 987, in _normalize_prompt_text_to_input
(APIServer pid=2355576) ERROR 02-17 00:46:50 [serving.py:323]     return self._validate_input(request, input_ids, input_text)
(APIServer pid=2355576) ERROR 02-17 00:46:50 [serving.py:323]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=2355576) ERROR 02-17 00:46:50 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1074, in _validate_input
(APIServer pid=2355576) ERROR 02-17 00:46:50 [serving.py:323]     raise VLLMValidationError(
(APIServer pid=2355576) ERROR 02-17 00:46:50 [serving.py:323] vllm.exceptions.VLLMValidationError: This model's maximum context length is 32768 tokens. However, your request has 33119 input tokens. Please reduce the length of the input messages. (parameter=input_tokens, value=33119)
(APIServer pid=2355576) INFO:     127.0.0.1:49974 - "POST /v1/chat/completions HTTP/1.1" 400 Bad Request
(APIServer pid=2355576) WARNING 02-17 00:46:50 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) ERROR 02-17 00:46:50 [serving.py:323] Error in preprocessing prompt inputs
(APIServer pid=2355576) ERROR 02-17 00:46:50 [serving.py:323] Traceback (most recent call last):
(APIServer pid=2355576) ERROR 02-17 00:46:50 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/chat_completion/serving.py", line 301, in render_chat_request
(APIServer pid=2355576) ERROR 02-17 00:46:50 [serving.py:323]     conversation, engine_prompts = await self._preprocess_chat(
(APIServer pid=2355576) ERROR 02-17 00:46:50 [serving.py:323]                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=2355576) ERROR 02-17 00:46:50 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1219, in _preprocess_chat
(APIServer pid=2355576) ERROR 02-17 00:46:50 [serving.py:323]     engine_prompt = await self._tokenize_prompt_input_async(
(APIServer pid=2355576) ERROR 02-17 00:46:50 [serving.py:323]                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=2355576) ERROR 02-17 00:46:50 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1106, in _tokenize_prompt_input_async
(APIServer pid=2355576) ERROR 02-17 00:46:50 [serving.py:323]     async for result in self._tokenize_prompt_inputs_async(
(APIServer pid=2355576) ERROR 02-17 00:46:50 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1127, in _tokenize_prompt_inputs_async
(APIServer pid=2355576) ERROR 02-17 00:46:50 [serving.py:323]     yield await self._normalize_prompt_text_to_input(
(APIServer pid=2355576) ERROR 02-17 00:46:50 [serving.py:323]           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=2355576) ERROR 02-17 00:46:50 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 987, in _normalize_prompt_text_to_input
(APIServer pid=2355576) ERROR 02-17 00:46:50 [serving.py:323]     return self._validate_input(request, input_ids, input_text)
(APIServer pid=2355576) ERROR 02-17 00:46:50 [serving.py:323]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=2355576) ERROR 02-17 00:46:50 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1074, in _validate_input
(APIServer pid=2355576) ERROR 02-17 00:46:50 [serving.py:323]     raise VLLMValidationError(
(APIServer pid=2355576) ERROR 02-17 00:46:50 [serving.py:323] vllm.exceptions.VLLMValidationError: This model's maximum context length is 32768 tokens. However, your request has 33119 input tokens. Please reduce the length of the input messages. (parameter=input_tokens, value=33119)
(APIServer pid=2355576) INFO:     127.0.0.1:49978 - "POST /v1/chat/completions HTTP/1.1" 400 Bad Request
(APIServer pid=2355576) WARNING 02-17 00:46:50 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) ERROR 02-17 00:46:50 [serving.py:323] Error in preprocessing prompt inputs
(APIServer pid=2355576) ERROR 02-17 00:46:50 [serving.py:323] Traceback (most recent call last):
(APIServer pid=2355576) ERROR 02-17 00:46:50 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/chat_completion/serving.py", line 301, in render_chat_request
(APIServer pid=2355576) ERROR 02-17 00:46:50 [serving.py:323]     conversation, engine_prompts = await self._preprocess_chat(
(APIServer pid=2355576) ERROR 02-17 00:46:50 [serving.py:323]                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=2355576) ERROR 02-17 00:46:50 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1219, in _preprocess_chat
(APIServer pid=2355576) ERROR 02-17 00:46:50 [serving.py:323]     engine_prompt = await self._tokenize_prompt_input_async(
(APIServer pid=2355576) ERROR 02-17 00:46:50 [serving.py:323]                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=2355576) ERROR 02-17 00:46:50 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1106, in _tokenize_prompt_input_async
(APIServer pid=2355576) ERROR 02-17 00:46:50 [serving.py:323]     async for result in self._tokenize_prompt_inputs_async(
(APIServer pid=2355576) ERROR 02-17 00:46:50 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1127, in _tokenize_prompt_inputs_async
(APIServer pid=2355576) ERROR 02-17 00:46:50 [serving.py:323]     yield await self._normalize_prompt_text_to_input(
(APIServer pid=2355576) ERROR 02-17 00:46:50 [serving.py:323]           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=2355576) ERROR 02-17 00:46:50 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 987, in _normalize_prompt_text_to_input
(APIServer pid=2355576) ERROR 02-17 00:46:50 [serving.py:323]     return self._validate_input(request, input_ids, input_text)
(APIServer pid=2355576) ERROR 02-17 00:46:50 [serving.py:323]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=2355576) ERROR 02-17 00:46:50 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1074, in _validate_input
(APIServer pid=2355576) ERROR 02-17 00:46:50 [serving.py:323]     raise VLLMValidationError(
(APIServer pid=2355576) ERROR 02-17 00:46:50 [serving.py:323] vllm.exceptions.VLLMValidationError: This model's maximum context length is 32768 tokens. However, your request has 33119 input tokens. Please reduce the length of the input messages. (parameter=input_tokens, value=33119)
(APIServer pid=2355576) INFO:     127.0.0.1:49980 - "POST /v1/chat/completions HTTP/1.1" 400 Bad Request
(APIServer pid=2355576) WARNING 02-17 00:46:50 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) ERROR 02-17 00:46:50 [serving.py:323] Error in preprocessing prompt inputs
(APIServer pid=2355576) ERROR 02-17 00:46:50 [serving.py:323] Traceback (most recent call last):
(APIServer pid=2355576) ERROR 02-17 00:46:50 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/chat_completion/serving.py", line 301, in render_chat_request
(APIServer pid=2355576) ERROR 02-17 00:46:50 [serving.py:323]     conversation, engine_prompts = await self._preprocess_chat(
(APIServer pid=2355576) ERROR 02-17 00:46:50 [serving.py:323]                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=2355576) ERROR 02-17 00:46:50 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1219, in _preprocess_chat
(APIServer pid=2355576) ERROR 02-17 00:46:50 [serving.py:323]     engine_prompt = await self._tokenize_prompt_input_async(
(APIServer pid=2355576) ERROR 02-17 00:46:50 [serving.py:323]                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=2355576) ERROR 02-17 00:46:50 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1106, in _tokenize_prompt_input_async
(APIServer pid=2355576) ERROR 02-17 00:46:50 [serving.py:323]     async for result in self._tokenize_prompt_inputs_async(
(APIServer pid=2355576) ERROR 02-17 00:46:50 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1127, in _tokenize_prompt_inputs_async
(APIServer pid=2355576) ERROR 02-17 00:46:50 [serving.py:323]     yield await self._normalize_prompt_text_to_input(
(APIServer pid=2355576) ERROR 02-17 00:46:50 [serving.py:323]           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=2355576) ERROR 02-17 00:46:50 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 987, in _normalize_prompt_text_to_input
(APIServer pid=2355576) ERROR 02-17 00:46:50 [serving.py:323]     return self._validate_input(request, input_ids, input_text)
(APIServer pid=2355576) ERROR 02-17 00:46:50 [serving.py:323]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=2355576) ERROR 02-17 00:46:50 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1074, in _validate_input
(APIServer pid=2355576) ERROR 02-17 00:46:50 [serving.py:323]     raise VLLMValidationError(
(APIServer pid=2355576) ERROR 02-17 00:46:50 [serving.py:323] vllm.exceptions.VLLMValidationError: This model's maximum context length is 32768 tokens. However, your request has 33119 input tokens. Please reduce the length of the input messages. (parameter=input_tokens, value=33119)
(APIServer pid=2355576) INFO:     127.0.0.1:49992 - "POST /v1/chat/completions HTTP/1.1" 400 Bad Request
(APIServer pid=2355576) WARNING 02-17 00:46:50 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:49994 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) INFO 02-17 00:46:54 [loggers.py:257] Engine 000: Avg prompt throughput: 452.0 tokens/s, Avg generation throughput: 325.8 tokens/s, Running: 3 reqs, Waiting: 0 reqs, GPU KV cache usage: 3.0%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) WARNING 02-17 00:46:54 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:34322 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:46:57 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:49956 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:47:02 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO 02-17 00:47:04 [loggers.py:257] Engine 000: Avg prompt throughput: 534.3 tokens/s, Avg generation throughput: 333.2 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 3.3%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) INFO:     127.0.0.1:34334 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:47:13 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO 02-17 00:47:14 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 419.3 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 3.5%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) INFO 02-17 00:47:24 [loggers.py:257] Engine 000: Avg prompt throughput: 205.6 tokens/s, Avg generation throughput: 429.5 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 3.7%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) INFO:     127.0.0.1:55674 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:47:33 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO 02-17 00:47:34 [loggers.py:257] Engine 000: Avg prompt throughput: 137.9 tokens/s, Avg generation throughput: 401.9 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 3.8%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) INFO:     127.0.0.1:41410 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:47:37 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:41416 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:47:42 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:60742 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:47:44 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO 02-17 00:47:44 [loggers.py:257] Engine 000: Avg prompt throughput: 272.8 tokens/s, Avg generation throughput: 361.7 tokens/s, Running: 3 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.1%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) ERROR 02-17 00:47:44 [serving.py:323] Error in preprocessing prompt inputs
(APIServer pid=2355576) ERROR 02-17 00:47:44 [serving.py:323] Traceback (most recent call last):
(APIServer pid=2355576) ERROR 02-17 00:47:44 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/chat_completion/serving.py", line 301, in render_chat_request
(APIServer pid=2355576) ERROR 02-17 00:47:44 [serving.py:323]     conversation, engine_prompts = await self._preprocess_chat(
(APIServer pid=2355576) ERROR 02-17 00:47:44 [serving.py:323]                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=2355576) ERROR 02-17 00:47:44 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1219, in _preprocess_chat
(APIServer pid=2355576) ERROR 02-17 00:47:44 [serving.py:323]     engine_prompt = await self._tokenize_prompt_input_async(
(APIServer pid=2355576) ERROR 02-17 00:47:44 [serving.py:323]                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=2355576) ERROR 02-17 00:47:44 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1106, in _tokenize_prompt_input_async
(APIServer pid=2355576) ERROR 02-17 00:47:44 [serving.py:323]     async for result in self._tokenize_prompt_inputs_async(
(APIServer pid=2355576) ERROR 02-17 00:47:44 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1127, in _tokenize_prompt_inputs_async
(APIServer pid=2355576) ERROR 02-17 00:47:44 [serving.py:323]     yield await self._normalize_prompt_text_to_input(
(APIServer pid=2355576) ERROR 02-17 00:47:44 [serving.py:323]           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=2355576) ERROR 02-17 00:47:44 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 987, in _normalize_prompt_text_to_input
(APIServer pid=2355576) ERROR 02-17 00:47:44 [serving.py:323]     return self._validate_input(request, input_ids, input_text)
(APIServer pid=2355576) ERROR 02-17 00:47:44 [serving.py:323]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=2355576) ERROR 02-17 00:47:44 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1084, in _validate_input
(APIServer pid=2355576) ERROR 02-17 00:47:44 [serving.py:323]     raise VLLMValidationError(
(APIServer pid=2355576) ERROR 02-17 00:47:44 [serving.py:323] vllm.exceptions.VLLMValidationError: 'max_tokens' or 'max_completion_tokens' is too large: 16384. This model's maximum context length is 32768 tokens and your request has 30765 input tokens (16384 > 32768 - 30765). (parameter=max_tokens, value=16384)
(APIServer pid=2355576) INFO:     127.0.0.1:49796 - "POST /v1/chat/completions HTTP/1.1" 400 Bad Request
(APIServer pid=2355576) WARNING 02-17 00:47:44 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) ERROR 02-17 00:47:44 [serving.py:323] Error in preprocessing prompt inputs
(APIServer pid=2355576) ERROR 02-17 00:47:44 [serving.py:323] Traceback (most recent call last):
(APIServer pid=2355576) ERROR 02-17 00:47:44 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/chat_completion/serving.py", line 301, in render_chat_request
(APIServer pid=2355576) ERROR 02-17 00:47:44 [serving.py:323]     conversation, engine_prompts = await self._preprocess_chat(
(APIServer pid=2355576) ERROR 02-17 00:47:44 [serving.py:323]                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=2355576) ERROR 02-17 00:47:44 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1219, in _preprocess_chat
(APIServer pid=2355576) ERROR 02-17 00:47:44 [serving.py:323]     engine_prompt = await self._tokenize_prompt_input_async(
(APIServer pid=2355576) ERROR 02-17 00:47:44 [serving.py:323]                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=2355576) ERROR 02-17 00:47:44 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1106, in _tokenize_prompt_input_async
(APIServer pid=2355576) ERROR 02-17 00:47:44 [serving.py:323]     async for result in self._tokenize_prompt_inputs_async(
(APIServer pid=2355576) ERROR 02-17 00:47:44 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1127, in _tokenize_prompt_inputs_async
(APIServer pid=2355576) ERROR 02-17 00:47:44 [serving.py:323]     yield await self._normalize_prompt_text_to_input(
(APIServer pid=2355576) ERROR 02-17 00:47:44 [serving.py:323]           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=2355576) ERROR 02-17 00:47:44 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 987, in _normalize_prompt_text_to_input
(APIServer pid=2355576) ERROR 02-17 00:47:44 [serving.py:323]     return self._validate_input(request, input_ids, input_text)
(APIServer pid=2355576) ERROR 02-17 00:47:44 [serving.py:323]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=2355576) ERROR 02-17 00:47:44 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1084, in _validate_input
(APIServer pid=2355576) ERROR 02-17 00:47:44 [serving.py:323]     raise VLLMValidationError(
(APIServer pid=2355576) ERROR 02-17 00:47:44 [serving.py:323] vllm.exceptions.VLLMValidationError: 'max_tokens' or 'max_completion_tokens' is too large: 16384. This model's maximum context length is 32768 tokens and your request has 30765 input tokens (16384 > 32768 - 30765). (parameter=max_tokens, value=16384)
(APIServer pid=2355576) INFO:     127.0.0.1:49804 - "POST /v1/chat/completions HTTP/1.1" 400 Bad Request
(APIServer pid=2355576) WARNING 02-17 00:47:44 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) ERROR 02-17 00:47:44 [serving.py:323] Error in preprocessing prompt inputs
(APIServer pid=2355576) ERROR 02-17 00:47:44 [serving.py:323] Traceback (most recent call last):
(APIServer pid=2355576) ERROR 02-17 00:47:44 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/chat_completion/serving.py", line 301, in render_chat_request
(APIServer pid=2355576) ERROR 02-17 00:47:44 [serving.py:323]     conversation, engine_prompts = await self._preprocess_chat(
(APIServer pid=2355576) ERROR 02-17 00:47:44 [serving.py:323]                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=2355576) ERROR 02-17 00:47:44 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1219, in _preprocess_chat
(APIServer pid=2355576) ERROR 02-17 00:47:44 [serving.py:323]     engine_prompt = await self._tokenize_prompt_input_async(
(APIServer pid=2355576) ERROR 02-17 00:47:44 [serving.py:323]                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=2355576) ERROR 02-17 00:47:44 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1106, in _tokenize_prompt_input_async
(APIServer pid=2355576) ERROR 02-17 00:47:44 [serving.py:323]     async for result in self._tokenize_prompt_inputs_async(
(APIServer pid=2355576) ERROR 02-17 00:47:44 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1127, in _tokenize_prompt_inputs_async
(APIServer pid=2355576) ERROR 02-17 00:47:44 [serving.py:323]     yield await self._normalize_prompt_text_to_input(
(APIServer pid=2355576) ERROR 02-17 00:47:44 [serving.py:323]           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=2355576) ERROR 02-17 00:47:44 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 987, in _normalize_prompt_text_to_input
(APIServer pid=2355576) ERROR 02-17 00:47:44 [serving.py:323]     return self._validate_input(request, input_ids, input_text)
(APIServer pid=2355576) ERROR 02-17 00:47:44 [serving.py:323]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=2355576) ERROR 02-17 00:47:44 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1084, in _validate_input
(APIServer pid=2355576) ERROR 02-17 00:47:44 [serving.py:323]     raise VLLMValidationError(
(APIServer pid=2355576) ERROR 02-17 00:47:44 [serving.py:323] vllm.exceptions.VLLMValidationError: 'max_tokens' or 'max_completion_tokens' is too large: 16384. This model's maximum context length is 32768 tokens and your request has 30765 input tokens (16384 > 32768 - 30765). (parameter=max_tokens, value=16384)
(APIServer pid=2355576) INFO:     127.0.0.1:49814 - "POST /v1/chat/completions HTTP/1.1" 400 Bad Request
(APIServer pid=2355576) WARNING 02-17 00:47:44 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) ERROR 02-17 00:47:44 [serving.py:323] Error in preprocessing prompt inputs
(APIServer pid=2355576) ERROR 02-17 00:47:44 [serving.py:323] Traceback (most recent call last):
(APIServer pid=2355576) ERROR 02-17 00:47:44 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/chat_completion/serving.py", line 301, in render_chat_request
(APIServer pid=2355576) ERROR 02-17 00:47:44 [serving.py:323]     conversation, engine_prompts = await self._preprocess_chat(
(APIServer pid=2355576) ERROR 02-17 00:47:44 [serving.py:323]                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=2355576) ERROR 02-17 00:47:44 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1219, in _preprocess_chat
(APIServer pid=2355576) ERROR 02-17 00:47:44 [serving.py:323]     engine_prompt = await self._tokenize_prompt_input_async(
(APIServer pid=2355576) ERROR 02-17 00:47:44 [serving.py:323]                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=2355576) ERROR 02-17 00:47:44 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1106, in _tokenize_prompt_input_async
(APIServer pid=2355576) ERROR 02-17 00:47:44 [serving.py:323]     async for result in self._tokenize_prompt_inputs_async(
(APIServer pid=2355576) ERROR 02-17 00:47:44 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1127, in _tokenize_prompt_inputs_async
(APIServer pid=2355576) ERROR 02-17 00:47:44 [serving.py:323]     yield await self._normalize_prompt_text_to_input(
(APIServer pid=2355576) ERROR 02-17 00:47:44 [serving.py:323]           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=2355576) ERROR 02-17 00:47:44 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 987, in _normalize_prompt_text_to_input
(APIServer pid=2355576) ERROR 02-17 00:47:44 [serving.py:323]     return self._validate_input(request, input_ids, input_text)
(APIServer pid=2355576) ERROR 02-17 00:47:44 [serving.py:323]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=2355576) ERROR 02-17 00:47:44 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1084, in _validate_input
(APIServer pid=2355576) ERROR 02-17 00:47:44 [serving.py:323]     raise VLLMValidationError(
(APIServer pid=2355576) ERROR 02-17 00:47:44 [serving.py:323] vllm.exceptions.VLLMValidationError: 'max_tokens' or 'max_completion_tokens' is too large: 16384. This model's maximum context length is 32768 tokens and your request has 30765 input tokens (16384 > 32768 - 30765). (parameter=max_tokens, value=16384)
(APIServer pid=2355576) INFO:     127.0.0.1:49828 - "POST /v1/chat/completions HTTP/1.1" 400 Bad Request
(APIServer pid=2355576) WARNING 02-17 00:47:44 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) ERROR 02-17 00:47:44 [serving.py:323] Error in preprocessing prompt inputs
(APIServer pid=2355576) ERROR 02-17 00:47:44 [serving.py:323] Traceback (most recent call last):
(APIServer pid=2355576) ERROR 02-17 00:47:44 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/chat_completion/serving.py", line 301, in render_chat_request
(APIServer pid=2355576) ERROR 02-17 00:47:44 [serving.py:323]     conversation, engine_prompts = await self._preprocess_chat(
(APIServer pid=2355576) ERROR 02-17 00:47:44 [serving.py:323]                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=2355576) ERROR 02-17 00:47:44 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1219, in _preprocess_chat
(APIServer pid=2355576) ERROR 02-17 00:47:44 [serving.py:323]     engine_prompt = await self._tokenize_prompt_input_async(
(APIServer pid=2355576) ERROR 02-17 00:47:44 [serving.py:323]                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=2355576) ERROR 02-17 00:47:44 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1106, in _tokenize_prompt_input_async
(APIServer pid=2355576) ERROR 02-17 00:47:44 [serving.py:323]     async for result in self._tokenize_prompt_inputs_async(
(APIServer pid=2355576) ERROR 02-17 00:47:44 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1127, in _tokenize_prompt_inputs_async
(APIServer pid=2355576) ERROR 02-17 00:47:44 [serving.py:323]     yield await self._normalize_prompt_text_to_input(
(APIServer pid=2355576) ERROR 02-17 00:47:44 [serving.py:323]           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=2355576) ERROR 02-17 00:47:44 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 987, in _normalize_prompt_text_to_input
(APIServer pid=2355576) ERROR 02-17 00:47:44 [serving.py:323]     return self._validate_input(request, input_ids, input_text)
(APIServer pid=2355576) ERROR 02-17 00:47:44 [serving.py:323]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=2355576) ERROR 02-17 00:47:44 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1084, in _validate_input
(APIServer pid=2355576) ERROR 02-17 00:47:44 [serving.py:323]     raise VLLMValidationError(
(APIServer pid=2355576) ERROR 02-17 00:47:44 [serving.py:323] vllm.exceptions.VLLMValidationError: 'max_tokens' or 'max_completion_tokens' is too large: 16384. This model's maximum context length is 32768 tokens and your request has 30765 input tokens (16384 > 32768 - 30765). (parameter=max_tokens, value=16384)
(APIServer pid=2355576) INFO:     127.0.0.1:49834 - "POST /v1/chat/completions HTTP/1.1" 400 Bad Request
(APIServer pid=2355576) WARNING 02-17 00:47:44 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) ERROR 02-17 00:47:44 [serving.py:323] Error in preprocessing prompt inputs
(APIServer pid=2355576) ERROR 02-17 00:47:44 [serving.py:323] Traceback (most recent call last):
(APIServer pid=2355576) ERROR 02-17 00:47:44 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/chat_completion/serving.py", line 301, in render_chat_request
(APIServer pid=2355576) ERROR 02-17 00:47:44 [serving.py:323]     conversation, engine_prompts = await self._preprocess_chat(
(APIServer pid=2355576) ERROR 02-17 00:47:44 [serving.py:323]                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=2355576) ERROR 02-17 00:47:44 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1219, in _preprocess_chat
(APIServer pid=2355576) ERROR 02-17 00:47:44 [serving.py:323]     engine_prompt = await self._tokenize_prompt_input_async(
(APIServer pid=2355576) ERROR 02-17 00:47:44 [serving.py:323]                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=2355576) ERROR 02-17 00:47:44 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1106, in _tokenize_prompt_input_async
(APIServer pid=2355576) ERROR 02-17 00:47:44 [serving.py:323]     async for result in self._tokenize_prompt_inputs_async(
(APIServer pid=2355576) ERROR 02-17 00:47:44 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1127, in _tokenize_prompt_inputs_async
(APIServer pid=2355576) ERROR 02-17 00:47:44 [serving.py:323]     yield await self._normalize_prompt_text_to_input(
(APIServer pid=2355576) ERROR 02-17 00:47:44 [serving.py:323]           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=2355576) ERROR 02-17 00:47:44 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 987, in _normalize_prompt_text_to_input
(APIServer pid=2355576) ERROR 02-17 00:47:44 [serving.py:323]     return self._validate_input(request, input_ids, input_text)
(APIServer pid=2355576) ERROR 02-17 00:47:44 [serving.py:323]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=2355576) ERROR 02-17 00:47:44 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1084, in _validate_input
(APIServer pid=2355576) ERROR 02-17 00:47:44 [serving.py:323]     raise VLLMValidationError(
(APIServer pid=2355576) ERROR 02-17 00:47:44 [serving.py:323] vllm.exceptions.VLLMValidationError: 'max_tokens' or 'max_completion_tokens' is too large: 16384. This model's maximum context length is 32768 tokens and your request has 30765 input tokens (16384 > 32768 - 30765). (parameter=max_tokens, value=16384)
(APIServer pid=2355576) INFO:     127.0.0.1:49844 - "POST /v1/chat/completions HTTP/1.1" 400 Bad Request
(APIServer pid=2355576) INFO:     127.0.0.1:49780 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:47:46 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) WARNING 02-17 00:47:46 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:49858 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:47:49 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:49864 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) INFO 02-17 00:47:54 [loggers.py:257] Engine 000: Avg prompt throughput: 343.2 tokens/s, Avg generation throughput: 361.6 tokens/s, Running: 3 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.3%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) WARNING 02-17 00:47:55 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:49860 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:47:58 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:37142 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) INFO:     127.0.0.1:37148 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) INFO:     127.0.0.1:60018 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:48:01 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) WARNING 02-17 00:48:02 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) WARNING 02-17 00:48:02 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO 02-17 00:48:04 [loggers.py:257] Engine 000: Avg prompt throughput: 1155.3 tokens/s, Avg generation throughput: 264.3 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.3%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) INFO:     127.0.0.1:57892 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) INFO:     127.0.0.1:51064 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) INFO:     127.0.0.1:37172 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:48:07 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) WARNING 02-17 00:48:07 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) ERROR 02-17 00:48:07 [serving.py:323] Error in preprocessing prompt inputs
(APIServer pid=2355576) ERROR 02-17 00:48:07 [serving.py:323] Traceback (most recent call last):
(APIServer pid=2355576) ERROR 02-17 00:48:07 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/chat_completion/serving.py", line 301, in render_chat_request
(APIServer pid=2355576) ERROR 02-17 00:48:07 [serving.py:323]     conversation, engine_prompts = await self._preprocess_chat(
(APIServer pid=2355576) ERROR 02-17 00:48:07 [serving.py:323]                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=2355576) ERROR 02-17 00:48:07 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1219, in _preprocess_chat
(APIServer pid=2355576) ERROR 02-17 00:48:07 [serving.py:323]     engine_prompt = await self._tokenize_prompt_input_async(
(APIServer pid=2355576) ERROR 02-17 00:48:07 [serving.py:323]                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=2355576) ERROR 02-17 00:48:07 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1106, in _tokenize_prompt_input_async
(APIServer pid=2355576) ERROR 02-17 00:48:07 [serving.py:323]     async for result in self._tokenize_prompt_inputs_async(
(APIServer pid=2355576) ERROR 02-17 00:48:07 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1127, in _tokenize_prompt_inputs_async
(APIServer pid=2355576) ERROR 02-17 00:48:07 [serving.py:323]     yield await self._normalize_prompt_text_to_input(
(APIServer pid=2355576) ERROR 02-17 00:48:07 [serving.py:323]           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=2355576) ERROR 02-17 00:48:07 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 987, in _normalize_prompt_text_to_input
(APIServer pid=2355576) ERROR 02-17 00:48:07 [serving.py:323]     return self._validate_input(request, input_ids, input_text)
(APIServer pid=2355576) ERROR 02-17 00:48:07 [serving.py:323]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=2355576) ERROR 02-17 00:48:07 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1084, in _validate_input
(APIServer pid=2355576) ERROR 02-17 00:48:07 [serving.py:323]     raise VLLMValidationError(
(APIServer pid=2355576) ERROR 02-17 00:48:07 [serving.py:323] vllm.exceptions.VLLMValidationError: 'max_tokens' or 'max_completion_tokens' is too large: 16384. This model's maximum context length is 32768 tokens and your request has 28167 input tokens (16384 > 32768 - 28167). (parameter=max_tokens, value=16384)
(APIServer pid=2355576) INFO:     127.0.0.1:51084 - "POST /v1/chat/completions HTTP/1.1" 400 Bad Request
(APIServer pid=2355576) WARNING 02-17 00:48:07 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) ERROR 02-17 00:48:07 [serving.py:323] Error in preprocessing prompt inputs
(APIServer pid=2355576) ERROR 02-17 00:48:07 [serving.py:323] Traceback (most recent call last):
(APIServer pid=2355576) ERROR 02-17 00:48:07 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/chat_completion/serving.py", line 301, in render_chat_request
(APIServer pid=2355576) ERROR 02-17 00:48:07 [serving.py:323]     conversation, engine_prompts = await self._preprocess_chat(
(APIServer pid=2355576) ERROR 02-17 00:48:07 [serving.py:323]                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=2355576) ERROR 02-17 00:48:07 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1219, in _preprocess_chat
(APIServer pid=2355576) ERROR 02-17 00:48:07 [serving.py:323]     engine_prompt = await self._tokenize_prompt_input_async(
(APIServer pid=2355576) ERROR 02-17 00:48:07 [serving.py:323]                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=2355576) ERROR 02-17 00:48:07 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1106, in _tokenize_prompt_input_async
(APIServer pid=2355576) ERROR 02-17 00:48:07 [serving.py:323]     async for result in self._tokenize_prompt_inputs_async(
(APIServer pid=2355576) ERROR 02-17 00:48:07 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1127, in _tokenize_prompt_inputs_async
(APIServer pid=2355576) ERROR 02-17 00:48:07 [serving.py:323]     yield await self._normalize_prompt_text_to_input(
(APIServer pid=2355576) ERROR 02-17 00:48:07 [serving.py:323]           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=2355576) ERROR 02-17 00:48:07 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 987, in _normalize_prompt_text_to_input
(APIServer pid=2355576) ERROR 02-17 00:48:07 [serving.py:323]     return self._validate_input(request, input_ids, input_text)
(APIServer pid=2355576) ERROR 02-17 00:48:07 [serving.py:323]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=2355576) ERROR 02-17 00:48:07 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1084, in _validate_input
(APIServer pid=2355576) ERROR 02-17 00:48:07 [serving.py:323]     raise VLLMValidationError(
(APIServer pid=2355576) ERROR 02-17 00:48:07 [serving.py:323] vllm.exceptions.VLLMValidationError: 'max_tokens' or 'max_completion_tokens' is too large: 16384. This model's maximum context length is 32768 tokens and your request has 28167 input tokens (16384 > 32768 - 28167). (parameter=max_tokens, value=16384)
(APIServer pid=2355576) INFO:     127.0.0.1:51090 - "POST /v1/chat/completions HTTP/1.1" 400 Bad Request
(APIServer pid=2355576) WARNING 02-17 00:48:08 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) ERROR 02-17 00:48:08 [serving.py:323] Error in preprocessing prompt inputs
(APIServer pid=2355576) ERROR 02-17 00:48:08 [serving.py:323] Traceback (most recent call last):
(APIServer pid=2355576) ERROR 02-17 00:48:08 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/chat_completion/serving.py", line 301, in render_chat_request
(APIServer pid=2355576) ERROR 02-17 00:48:08 [serving.py:323]     conversation, engine_prompts = await self._preprocess_chat(
(APIServer pid=2355576) ERROR 02-17 00:48:08 [serving.py:323]                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=2355576) ERROR 02-17 00:48:08 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1219, in _preprocess_chat
(APIServer pid=2355576) ERROR 02-17 00:48:08 [serving.py:323]     engine_prompt = await self._tokenize_prompt_input_async(
(APIServer pid=2355576) ERROR 02-17 00:48:08 [serving.py:323]                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=2355576) ERROR 02-17 00:48:08 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1106, in _tokenize_prompt_input_async
(APIServer pid=2355576) ERROR 02-17 00:48:08 [serving.py:323]     async for result in self._tokenize_prompt_inputs_async(
(APIServer pid=2355576) ERROR 02-17 00:48:08 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1127, in _tokenize_prompt_inputs_async
(APIServer pid=2355576) ERROR 02-17 00:48:08 [serving.py:323]     yield await self._normalize_prompt_text_to_input(
(APIServer pid=2355576) ERROR 02-17 00:48:08 [serving.py:323]           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=2355576) ERROR 02-17 00:48:08 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 987, in _normalize_prompt_text_to_input
(APIServer pid=2355576) ERROR 02-17 00:48:08 [serving.py:323]     return self._validate_input(request, input_ids, input_text)
(APIServer pid=2355576) ERROR 02-17 00:48:08 [serving.py:323]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=2355576) ERROR 02-17 00:48:08 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1084, in _validate_input
(APIServer pid=2355576) ERROR 02-17 00:48:08 [serving.py:323]     raise VLLMValidationError(
(APIServer pid=2355576) ERROR 02-17 00:48:08 [serving.py:323] vllm.exceptions.VLLMValidationError: 'max_tokens' or 'max_completion_tokens' is too large: 16384. This model's maximum context length is 32768 tokens and your request has 28167 input tokens (16384 > 32768 - 28167). (parameter=max_tokens, value=16384)
(APIServer pid=2355576) INFO:     127.0.0.1:51096 - "POST /v1/chat/completions HTTP/1.1" 400 Bad Request
(APIServer pid=2355576) WARNING 02-17 00:48:08 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) ERROR 02-17 00:48:08 [serving.py:323] Error in preprocessing prompt inputs
(APIServer pid=2355576) ERROR 02-17 00:48:08 [serving.py:323] Traceback (most recent call last):
(APIServer pid=2355576) ERROR 02-17 00:48:08 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/chat_completion/serving.py", line 301, in render_chat_request
(APIServer pid=2355576) ERROR 02-17 00:48:08 [serving.py:323]     conversation, engine_prompts = await self._preprocess_chat(
(APIServer pid=2355576) ERROR 02-17 00:48:08 [serving.py:323]                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=2355576) ERROR 02-17 00:48:08 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1219, in _preprocess_chat
(APIServer pid=2355576) ERROR 02-17 00:48:08 [serving.py:323]     engine_prompt = await self._tokenize_prompt_input_async(
(APIServer pid=2355576) ERROR 02-17 00:48:08 [serving.py:323]                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=2355576) ERROR 02-17 00:48:08 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1106, in _tokenize_prompt_input_async
(APIServer pid=2355576) ERROR 02-17 00:48:08 [serving.py:323]     async for result in self._tokenize_prompt_inputs_async(
(APIServer pid=2355576) ERROR 02-17 00:48:08 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1127, in _tokenize_prompt_inputs_async
(APIServer pid=2355576) ERROR 02-17 00:48:08 [serving.py:323]     yield await self._normalize_prompt_text_to_input(
(APIServer pid=2355576) ERROR 02-17 00:48:08 [serving.py:323]           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=2355576) ERROR 02-17 00:48:08 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 987, in _normalize_prompt_text_to_input
(APIServer pid=2355576) ERROR 02-17 00:48:08 [serving.py:323]     return self._validate_input(request, input_ids, input_text)
(APIServer pid=2355576) ERROR 02-17 00:48:08 [serving.py:323]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=2355576) ERROR 02-17 00:48:08 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1084, in _validate_input
(APIServer pid=2355576) ERROR 02-17 00:48:08 [serving.py:323]     raise VLLMValidationError(
(APIServer pid=2355576) ERROR 02-17 00:48:08 [serving.py:323] vllm.exceptions.VLLMValidationError: 'max_tokens' or 'max_completion_tokens' is too large: 16384. This model's maximum context length is 32768 tokens and your request has 28167 input tokens (16384 > 32768 - 28167). (parameter=max_tokens, value=16384)
(APIServer pid=2355576) INFO:     127.0.0.1:51110 - "POST /v1/chat/completions HTTP/1.1" 400 Bad Request
(APIServer pid=2355576) WARNING 02-17 00:48:08 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) WARNING 02-17 00:48:08 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) ERROR 02-17 00:48:08 [serving.py:323] Error in preprocessing prompt inputs
(APIServer pid=2355576) ERROR 02-17 00:48:08 [serving.py:323] Traceback (most recent call last):
(APIServer pid=2355576) ERROR 02-17 00:48:08 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/chat_completion/serving.py", line 301, in render_chat_request
(APIServer pid=2355576) ERROR 02-17 00:48:08 [serving.py:323]     conversation, engine_prompts = await self._preprocess_chat(
(APIServer pid=2355576) ERROR 02-17 00:48:08 [serving.py:323]                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=2355576) ERROR 02-17 00:48:08 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1219, in _preprocess_chat
(APIServer pid=2355576) ERROR 02-17 00:48:08 [serving.py:323]     engine_prompt = await self._tokenize_prompt_input_async(
(APIServer pid=2355576) ERROR 02-17 00:48:08 [serving.py:323]                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=2355576) ERROR 02-17 00:48:08 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1106, in _tokenize_prompt_input_async
(APIServer pid=2355576) ERROR 02-17 00:48:08 [serving.py:323]     async for result in self._tokenize_prompt_inputs_async(
(APIServer pid=2355576) ERROR 02-17 00:48:08 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1127, in _tokenize_prompt_inputs_async
(APIServer pid=2355576) ERROR 02-17 00:48:08 [serving.py:323]     yield await self._normalize_prompt_text_to_input(
(APIServer pid=2355576) ERROR 02-17 00:48:08 [serving.py:323]           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=2355576) ERROR 02-17 00:48:08 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 987, in _normalize_prompt_text_to_input
(APIServer pid=2355576) ERROR 02-17 00:48:08 [serving.py:323]     return self._validate_input(request, input_ids, input_text)
(APIServer pid=2355576) ERROR 02-17 00:48:08 [serving.py:323]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=2355576) ERROR 02-17 00:48:08 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1084, in _validate_input
(APIServer pid=2355576) ERROR 02-17 00:48:08 [serving.py:323]     raise VLLMValidationError(
(APIServer pid=2355576) ERROR 02-17 00:48:08 [serving.py:323] vllm.exceptions.VLLMValidationError: 'max_tokens' or 'max_completion_tokens' is too large: 16384. This model's maximum context length is 32768 tokens and your request has 28167 input tokens (16384 > 32768 - 28167). (parameter=max_tokens, value=16384)
(APIServer pid=2355576) INFO:     127.0.0.1:51118 - "POST /v1/chat/completions HTTP/1.1" 400 Bad Request
(APIServer pid=2355576) WARNING 02-17 00:48:08 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:51130 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) INFO:     127.0.0.1:51068 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:48:11 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) WARNING 02-17 00:48:13 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO 02-17 00:48:14 [loggers.py:257] Engine 000: Avg prompt throughput: 692.1 tokens/s, Avg generation throughput: 267.0 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.0%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) INFO:     127.0.0.1:51116 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:48:17 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:42636 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) INFO 02-17 00:48:24 [loggers.py:257] Engine 000: Avg prompt throughput: 450.8 tokens/s, Avg generation throughput: 388.6 tokens/s, Running: 3 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.1%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) WARNING 02-17 00:48:24 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:51138 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:48:32 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO 02-17 00:48:34 [loggers.py:257] Engine 000: Avg prompt throughput: 618.5 tokens/s, Avg generation throughput: 378.0 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.6%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) INFO:     127.0.0.1:57062 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:48:36 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:42630 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:48:38 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:42628 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:48:41 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:42654 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) INFO 02-17 00:48:44 [loggers.py:257] Engine 000: Avg prompt throughput: 248.8 tokens/s, Avg generation throughput: 401.7 tokens/s, Running: 3 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.1%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) WARNING 02-17 00:48:45 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:55602 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:48:49 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:42640 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:48:51 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:55612 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:48:52 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO 02-17 00:48:54 [loggers.py:257] Engine 000: Avg prompt throughput: 458.3 tokens/s, Avg generation throughput: 369.0 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.4%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) INFO:     127.0.0.1:41862 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) INFO:     127.0.0.1:42620 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:48:57 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) WARNING 02-17 00:48:58 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:41874 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:49:01 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO 02-17 00:49:04 [loggers.py:257] Engine 000: Avg prompt throughput: 317.9 tokens/s, Avg generation throughput: 369.1 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.4%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) INFO:     127.0.0.1:41896 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:49:07 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:41886 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:49:09 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:41474 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:49:10 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:41480 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) INFO:     127.0.0.1:55620 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:49:12 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) WARNING 02-17 00:49:13 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO 02-17 00:49:14 [loggers.py:257] Engine 000: Avg prompt throughput: 607.1 tokens/s, Avg generation throughput: 315.4 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.3%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) INFO:     127.0.0.1:37164 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) INFO:     127.0.0.1:41490 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:49:15 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) WARNING 02-17 00:49:16 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:52236 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:49:19 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:52222 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) INFO:     127.0.0.1:52246 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:49:22 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO 02-17 00:49:24 [loggers.py:257] Engine 000: Avg prompt throughput: 448.6 tokens/s, Avg generation throughput: 355.3 tokens/s, Running: 3 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.6%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) WARNING 02-17 00:49:24 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:51824 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:49:28 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:51828 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) INFO:     127.0.0.1:51822 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:49:32 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) WARNING 02-17 00:49:33 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO 02-17 00:49:34 [loggers.py:257] Engine 000: Avg prompt throughput: 324.0 tokens/s, Avg generation throughput: 350.0 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.9%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) INFO:     127.0.0.1:52212 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:49:36 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:41946 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:49:42 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO 02-17 00:49:44 [loggers.py:257] Engine 000: Avg prompt throughput: 715.1 tokens/s, Avg generation throughput: 396.2 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.0%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) INFO:     127.0.0.1:56368 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:49:46 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:41940 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:49:48 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:56376 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:49:51 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:56386 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) INFO 02-17 00:49:54 [loggers.py:257] Engine 000: Avg prompt throughput: 238.7 tokens/s, Avg generation throughput: 370.3 tokens/s, Running: 3 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.9%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) WARNING 02-17 00:49:55 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:48856 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:49:58 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:48866 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:50:02 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO 02-17 00:50:04 [loggers.py:257] Engine 000: Avg prompt throughput: 225.4 tokens/s, Avg generation throughput: 383.9 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.2%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) INFO:     127.0.0.1:36304 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) INFO:     127.0.0.1:41956 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:50:06 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) WARNING 02-17 00:50:06 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:52226 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) INFO:     127.0.0.1:36318 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:50:09 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) WARNING 02-17 00:50:10 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:56382 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) INFO 02-17 00:50:14 [loggers.py:257] Engine 000: Avg prompt throughput: 485.7 tokens/s, Avg generation throughput: 339.9 tokens/s, Running: 3 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.5%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) INFO:     127.0.0.1:36354 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:50:15 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) WARNING 02-17 00:50:17 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO 02-17 00:50:24 [loggers.py:257] Engine 000: Avg prompt throughput: 383.9 tokens/s, Avg generation throughput: 366.4 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.0%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) INFO:     127.0.0.1:57040 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:50:25 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:57036 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:50:26 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:42036 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:50:33 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO 02-17 00:50:34 [loggers.py:257] Engine 000: Avg prompt throughput: 386.3 tokens/s, Avg generation throughput: 413.8 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.0%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) INFO:     127.0.0.1:42052 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:50:40 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:38218 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:50:43 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:36330 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:50:44 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO 02-17 00:50:44 [loggers.py:257] Engine 000: Avg prompt throughput: 196.6 tokens/s, Avg generation throughput: 397.9 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.7%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) INFO:     127.0.0.1:53334 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:50:48 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:36342 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) INFO:     127.0.0.1:53360 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:50:53 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) WARNING 02-17 00:50:53 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO 02-17 00:50:54 [loggers.py:257] Engine 000: Avg prompt throughput: 700.3 tokens/s, Avg generation throughput: 323.3 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.8%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) INFO:     127.0.0.1:47510 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:50:57 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:38232 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:50:57 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:47522 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:51:01 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO 02-17 00:51:04 [loggers.py:257] Engine 000: Avg prompt throughput: 366.9 tokens/s, Avg generation throughput: 376.5 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.0%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) INFO:     127.0.0.1:47536 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:51:05 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:44884 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) INFO 02-17 00:51:14 [loggers.py:257] Engine 000: Avg prompt throughput: 74.9 tokens/s, Avg generation throughput: 367.9 tokens/s, Running: 3 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.1%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) INFO:     127.0.0.1:47528 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:51:17 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:55208 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:51:22 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO 02-17 00:51:24 [loggers.py:257] Engine 000: Avg prompt throughput: 214.0 tokens/s, Avg generation throughput: 312.4 tokens/s, Running: 3 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.1%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) INFO:     127.0.0.1:47508 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:51:24 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:53348 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:51:28 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:46210 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) INFO 02-17 00:51:34 [loggers.py:257] Engine 000: Avg prompt throughput: 593.2 tokens/s, Avg generation throughput: 269.5 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.6%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) WARNING 02-17 00:51:34 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:56576 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:51:42 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO 02-17 00:51:44 [loggers.py:257] Engine 000: Avg prompt throughput: 238.6 tokens/s, Avg generation throughput: 322.1 tokens/s, Running: 3 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.9%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) INFO:     127.0.0.1:46214 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) INFO:     127.0.0.1:43204 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:51:53 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO 02-17 00:51:54 [loggers.py:257] Engine 000: Avg prompt throughput: 203.6 tokens/s, Avg generation throughput: 306.6 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.7%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) WARNING 02-17 00:51:54 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:57580 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:52:00 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO 02-17 00:52:04 [loggers.py:257] Engine 000: Avg prompt throughput: 220.4 tokens/s, Avg generation throughput: 324.6 tokens/s, Running: 3 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.0%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) INFO:     127.0.0.1:46212 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:52:08 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:57590 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:52:11 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO 02-17 00:52:14 [loggers.py:257] Engine 000: Avg prompt throughput: 466.8 tokens/s, Avg generation throughput: 275.0 tokens/s, Running: 3 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.8%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) INFO:     127.0.0.1:50460 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:52:17 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:50446 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:52:22 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO 02-17 00:52:24 [loggers.py:257] Engine 000: Avg prompt throughput: 204.9 tokens/s, Avg generation throughput: 326.4 tokens/s, Running: 3 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.7%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) INFO:     127.0.0.1:39608 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) INFO:     127.0.0.1:56370 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:52:26 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) WARNING 02-17 00:52:28 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:57574 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) INFO:     127.0.0.1:39620 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) INFO 02-17 00:52:34 [loggers.py:257] Engine 000: Avg prompt throughput: 425.4 tokens/s, Avg generation throughput: 257.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.3%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) WARNING 02-17 00:52:34 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) WARNING 02-17 00:52:35 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:39636 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:52:40 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:51408 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) INFO 02-17 00:52:44 [loggers.py:257] Engine 000: Avg prompt throughput: 358.1 tokens/s, Avg generation throughput: 295.7 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.4%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) INFO:     127.0.0.1:51404 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:52:44 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(Worker_TP3 pid=2356195) /users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/model_executor/layers/fla/ops/utils.py:113: UserWarning: Input tensor shape suggests potential format mismatch: seq_len (4) < num_heads (8). This may indicate the inputs were passed in head-first format [B, H, T, ...] when head_first=False was specified. Please verify your input tensor format matches the expected shape [B, T, H, ...].
(Worker_TP3 pid=2356195)   return fn(*contiguous_args, **contiguous_kwargs)
(Worker_TP0 pid=2356192) /users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/model_executor/layers/fla/ops/utils.py:113: UserWarning: Input tensor shape suggests potential format mismatch: seq_len (4) < num_heads (8). This may indicate the inputs were passed in head-first format [B, H, T, ...] when head_first=False was specified. Please verify your input tensor format matches the expected shape [B, T, H, ...].
(Worker_TP0 pid=2356192)   return fn(*contiguous_args, **contiguous_kwargs)
(Worker_TP1 pid=2356193) /users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/model_executor/layers/fla/ops/utils.py:113: UserWarning: Input tensor shape suggests potential format mismatch: seq_len (4) < num_heads (8). This may indicate the inputs were passed in head-first format [B, H, T, ...] when head_first=False was specified. Please verify your input tensor format matches the expected shape [B, T, H, ...].
(Worker_TP1 pid=2356193)   return fn(*contiguous_args, **contiguous_kwargs)
(Worker_TP2 pid=2356194) /users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/model_executor/layers/fla/ops/utils.py:113: UserWarning: Input tensor shape suggests potential format mismatch: seq_len (4) < num_heads (8). This may indicate the inputs were passed in head-first format [B, H, T, ...] when head_first=False was specified. Please verify your input tensor format matches the expected shape [B, T, H, ...].
(Worker_TP2 pid=2356194)   return fn(*contiguous_args, **contiguous_kwargs)
(APIServer pid=2355576) WARNING 02-17 00:52:45 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:39136 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:52:49 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:39146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:52:52 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO 02-17 00:52:54 [loggers.py:257] Engine 000: Avg prompt throughput: 426.1 tokens/s, Avg generation throughput: 287.6 tokens/s, Running: 3 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.6%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) INFO:     127.0.0.1:39124 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:52:56 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:51480 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:52:58 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:51490 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:53:01 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:51506 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:53:03 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO 02-17 00:53:04 [loggers.py:257] Engine 000: Avg prompt throughput: 355.2 tokens/s, Avg generation throughput: 270.2 tokens/s, Running: 3 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.6%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) INFO:     127.0.0.1:35612 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:53:07 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:51518 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:53:09 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:35614 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:53:11 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:35630 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:53:13 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO 02-17 00:53:14 [loggers.py:257] Engine 000: Avg prompt throughput: 331.1 tokens/s, Avg generation throughput: 270.5 tokens/s, Running: 3 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.7%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) INFO:     127.0.0.1:35640 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:53:16 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:33318 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) INFO:     127.0.0.1:33334 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:53:19 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) WARNING 02-17 00:53:20 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:33348 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) INFO:     127.0.0.1:33338 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:53:23 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) WARNING 02-17 00:53:23 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO 02-17 00:53:24 [loggers.py:257] Engine 000: Avg prompt throughput: 376.8 tokens/s, Avg generation throughput: 236.2 tokens/s, Running: 3 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.7%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) INFO:     127.0.0.1:51396 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:53:25 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:38348 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) INFO:     127.0.0.1:38360 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:53:29 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) WARNING 02-17 00:53:29 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:38368 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) INFO 02-17 00:53:34 [loggers.py:257] Engine 000: Avg prompt throughput: 582.4 tokens/s, Avg generation throughput: 224.1 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.5%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) WARNING 02-17 00:53:34 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:38372 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:53:37 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:51614 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) INFO:     127.0.0.1:51622 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:53:40 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) WARNING 02-17 00:53:41 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:51646 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) INFO 02-17 00:53:44 [loggers.py:257] Engine 000: Avg prompt throughput: 308.3 tokens/s, Avg generation throughput: 288.9 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.4%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) INFO:     127.0.0.1:51634 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:53:45 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) WARNING 02-17 00:53:46 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:50724 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:53:47 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:50720 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) INFO:     127.0.0.1:50730 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:53:51 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) WARNING 02-17 00:53:52 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO 02-17 00:53:54 [loggers.py:257] Engine 000: Avg prompt throughput: 1649.7 tokens/s, Avg generation throughput: 129.1 tokens/s, Running: 3 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.9%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) INFO:     127.0.0.1:50732 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:53:59 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:57788 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) INFO:     127.0.0.1:38366 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:54:02 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) WARNING 02-17 00:54:03 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO 02-17 00:54:04 [loggers.py:257] Engine 000: Avg prompt throughput: 149.6 tokens/s, Avg generation throughput: 268.8 tokens/s, Running: 3 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.8%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) INFO:     127.0.0.1:41268 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:54:07 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:41290 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) INFO:     127.0.0.1:57776 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:54:10 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) WARNING 02-17 00:54:11 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:41282 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:54:12 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:41302 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) INFO 02-17 00:54:14 [loggers.py:257] Engine 000: Avg prompt throughput: 734.0 tokens/s, Avg generation throughput: 266.3 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.2%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) INFO:     127.0.0.1:41296 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:54:16 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) WARNING 02-17 00:54:17 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:46618 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:54:19 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:46632 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:54:21 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:46648 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) INFO 02-17 00:54:24 [loggers.py:257] Engine 000: Avg prompt throughput: 608.7 tokens/s, Avg generation throughput: 229.9 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.5%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) WARNING 02-17 00:54:25 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:46634 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:54:26 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:46646 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:54:29 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:47670 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:54:30 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:47660 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) INFO:     127.0.0.1:47674 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:54:32 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) WARNING 02-17 00:54:33 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO 02-17 00:54:34 [loggers.py:257] Engine 000: Avg prompt throughput: 514.9 tokens/s, Avg generation throughput: 256.4 tokens/s, Running: 3 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.4%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) INFO:     127.0.0.1:47684 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) INFO:     127.0.0.1:43374 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:54:36 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) WARNING 02-17 00:54:36 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:43380 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) INFO:     127.0.0.1:43382 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:54:41 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) WARNING 02-17 00:54:41 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO 02-17 00:54:44 [loggers.py:257] Engine 000: Avg prompt throughput: 373.6 tokens/s, Avg generation throughput: 276.6 tokens/s, Running: 3 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.4%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) INFO:     127.0.0.1:43412 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) INFO:     127.0.0.1:43388 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:54:47 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) WARNING 02-17 00:54:48 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:43404 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:54:50 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO 02-17 00:54:54 [loggers.py:257] Engine 000: Avg prompt throughput: 369.0 tokens/s, Avg generation throughput: 291.4 tokens/s, Running: 3 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.5%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) INFO:     127.0.0.1:47454 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) INFO:     127.0.0.1:47440 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:54:55 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) WARNING 02-17 00:54:56 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:37152 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:55:01 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:47426 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:55:04 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO 02-17 00:55:04 [loggers.py:257] Engine 000: Avg prompt throughput: 245.3 tokens/s, Avg generation throughput: 264.3 tokens/s, Running: 3 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.4%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) INFO:     127.0.0.1:37166 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) INFO:     127.0.0.1:37158 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:55:05 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) WARNING 02-17 00:55:06 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:38938 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:55:10 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:38928 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:55:10 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:38946 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) INFO:     127.0.0.1:38940 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) INFO 02-17 00:55:14 [loggers.py:257] Engine 000: Avg prompt throughput: 451.0 tokens/s, Avg generation throughput: 251.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.2%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) WARNING 02-17 00:55:15 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) WARNING 02-17 00:55:15 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:47068 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:55:17 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:47074 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) INFO:     127.0.0.1:47066 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) INFO:     127.0.0.1:38950 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:55:22 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) WARNING 02-17 00:55:23 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO 02-17 00:55:24 [loggers.py:257] Engine 000: Avg prompt throughput: 1023.3 tokens/s, Avg generation throughput: 112.3 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.4%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) INFO:     127.0.0.1:60916 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:55:24 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:60910 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:55:30 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO 02-17 00:55:34 [loggers.py:257] Engine 000: Avg prompt throughput: 1049.4 tokens/s, Avg generation throughput: 207.2 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.6%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) INFO:     127.0.0.1:60922 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) INFO:     127.0.0.1:60924 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:55:39 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) WARNING 02-17 00:55:39 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:38604 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:55:43 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO 02-17 00:55:44 [loggers.py:257] Engine 000: Avg prompt throughput: 359.9 tokens/s, Avg generation throughput: 127.7 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.5%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) INFO:     127.0.0.1:55416 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:55:44 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:55428 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) INFO:     127.0.0.1:38618 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:55:48 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) WARNING 02-17 00:55:48 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO 02-17 00:55:54 [loggers.py:257] Engine 000: Avg prompt throughput: 1214.9 tokens/s, Avg generation throughput: 174.5 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.4%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) INFO:     127.0.0.1:55448 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:55:59 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:55444 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:56:00 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:44460 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:56:03 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO 02-17 00:56:04 [loggers.py:257] Engine 000: Avg prompt throughput: 225.4 tokens/s, Avg generation throughput: 206.5 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.2%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) INFO:     127.0.0.1:44468 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:56:05 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:57714 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:56:07 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:57730 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) INFO:     127.0.0.1:57722 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:56:10 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) WARNING 02-17 00:56:12 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:57744 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) INFO 02-17 00:56:14 [loggers.py:257] Engine 000: Avg prompt throughput: 347.2 tokens/s, Avg generation throughput: 170.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.1%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) WARNING 02-17 00:56:14 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:55770 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:56:15 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:55780 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:56:18 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:55794 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) INFO:     127.0.0.1:55808 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:56:21 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) WARNING 02-17 00:56:22 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:32832 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) INFO 02-17 00:56:24 [loggers.py:257] Engine 000: Avg prompt throughput: 405.4 tokens/s, Avg generation throughput: 166.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.1%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) INFO:     127.0.0.1:55820 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:56:25 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:32846 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:56:29 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:32848 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:56:32 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO 02-17 00:56:34 [loggers.py:257] Engine 000: Avg prompt throughput: 274.5 tokens/s, Avg generation throughput: 112.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.1%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) INFO:     127.0.0.1:54178 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:56:36 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:54180 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:56:40 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:54192 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) INFO 02-17 00:56:44 [loggers.py:257] Engine 000: Avg prompt throughput: 157.5 tokens/s, Avg generation throughput: 86.2 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) WARNING 02-17 00:56:45 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:55766 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:56:50 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO 02-17 00:56:54 [loggers.py:257] Engine 000: Avg prompt throughput: 228.6 tokens/s, Avg generation throughput: 124.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.2%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) INFO:     127.0.0.1:55776 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:56:59 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO 02-17 00:57:04 [loggers.py:257] Engine 000: Avg prompt throughput: 83.4 tokens/s, Avg generation throughput: 122.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.1%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) INFO:     127.0.0.1:33660 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:57:05 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:46344 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) INFO 02-17 00:57:14 [loggers.py:257] Engine 000: Avg prompt throughput: 145.2 tokens/s, Avg generation throughput: 121.1 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) WARNING 02-17 00:57:14 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:53206 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:57:19 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO 02-17 00:57:24 [loggers.py:257] Engine 000: Avg prompt throughput: 223.4 tokens/s, Avg generation throughput: 135.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.2%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) INFO:     127.0.0.1:53208 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:57:27 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:35930 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:57:32 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO 02-17 00:57:34 [loggers.py:257] Engine 000: Avg prompt throughput: 473.0 tokens/s, Avg generation throughput: 56.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.3%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) INFO:     127.0.0.1:54328 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:57:41 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:54340 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) INFO 02-17 00:57:44 [loggers.py:257] Engine 000: Avg prompt throughput: 73.3 tokens/s, Avg generation throughput: 129.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) WARNING 02-17 00:57:44 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:57672 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:57:48 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:57676 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:57:52 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO 02-17 00:57:54 [loggers.py:257] Engine 000: Avg prompt throughput: 234.8 tokens/s, Avg generation throughput: 83.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.1%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) INFO:     127.0.0.1:47122 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:57:55 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:47124 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:57:59 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:47126 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:58:02 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO 02-17 00:58:04 [loggers.py:257] Engine 000: Avg prompt throughput: 226.6 tokens/s, Avg generation throughput: 68.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.1%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) INFO:     127.0.0.1:55648 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:58:06 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:55664 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:58:09 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:55676 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:58:12 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO 02-17 00:58:14 [loggers.py:257] Engine 000: Avg prompt throughput: 220.7 tokens/s, Avg generation throughput: 68.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.1%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) INFO:     127.0.0.1:35926 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:58:17 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO 02-17 00:58:24 [loggers.py:257] Engine 000: Avg prompt throughput: 332.8 tokens/s, Avg generation throughput: 85.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.3%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) INFO:     127.0.0.1:35942 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:58:25 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:41556 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:58:29 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:41564 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:58:32 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO 02-17 00:58:34 [loggers.py:257] Engine 000: Avg prompt throughput: 221.3 tokens/s, Avg generation throughput: 90.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.1%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) INFO:     127.0.0.1:53846 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:58:35 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:53854 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:58:40 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:53860 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:58:43 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO 02-17 00:58:44 [loggers.py:257] Engine 000: Avg prompt throughput: 229.8 tokens/s, Avg generation throughput: 68.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.1%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) INFO:     127.0.0.1:43996 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:58:49 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:43998 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:58:53 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO 02-17 00:58:54 [loggers.py:257] Engine 000: Avg prompt throughput: 205.9 tokens/s, Avg generation throughput: 115.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.1%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) INFO:     127.0.0.1:34114 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:59:00 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:34116 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:59:03 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO 02-17 00:59:04 [loggers.py:257] Engine 000: Avg prompt throughput: 72.8 tokens/s, Avg generation throughput: 95.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.1%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) INFO:     127.0.0.1:34986 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 00:59:07 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:34990 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) INFO 02-17 00:59:14 [loggers.py:257] Engine 000: Avg prompt throughput: 149.8 tokens/s, Avg generation throughput: 46.2 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) INFO 02-17 00:59:24 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) WARNING 02-17 01:24:38 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO 02-17 01:24:44 [loggers.py:257] Engine 000: Avg prompt throughput: 1113.6 tokens/s, Avg generation throughput: 40.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.8%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) INFO:     127.0.0.1:40074 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 01:24:51 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO 02-17 01:24:54 [loggers.py:257] Engine 000: Avg prompt throughput: 119.7 tokens/s, Avg generation throughput: 141.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.1%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) INFO:     127.0.0.1:56774 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) INFO 02-17 01:25:04 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 136.3 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) WARNING 02-17 01:25:04 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO 02-17 01:25:14 [loggers.py:257] Engine 000: Avg prompt throughput: 143.9 tokens/s, Avg generation throughput: 138.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.2%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) INFO:     127.0.0.1:54332 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) INFO 02-17 01:25:24 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.1 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) INFO 02-17 01:25:34 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) WARNING 02-17 01:35:28 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO 02-17 01:35:34 [loggers.py:257] Engine 000: Avg prompt throughput: 1113.6 tokens/s, Avg generation throughput: 42.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.8%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) INFO 02-17 01:35:44 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 147.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.8%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) INFO 02-17 01:35:54 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 147.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.9%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) INFO:     127.0.0.1:45050 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 01:35:59 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO 02-17 01:36:04 [loggers.py:257] Engine 000: Avg prompt throughput: 119.9 tokens/s, Avg generation throughput: 141.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.2%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) INFO:     127.0.0.1:46948 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 01:36:13 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO 02-17 01:36:14 [loggers.py:257] Engine 000: Avg prompt throughput: 110.1 tokens/s, Avg generation throughput: 124.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.1%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) INFO:     127.0.0.1:57430 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 01:36:22 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO 02-17 01:36:24 [loggers.py:257] Engine 000: Avg prompt throughput: 196.4 tokens/s, Avg generation throughput: 141.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.2%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) INFO 02-17 01:36:34 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 149.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.3%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) INFO:     127.0.0.1:35020 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 01:36:35 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO 02-17 01:36:44 [loggers.py:257] Engine 000: Avg prompt throughput: 144.0 tokens/s, Avg generation throughput: 126.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.2%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) INFO:     127.0.0.1:33570 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 01:36:48 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:34496 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 01:36:52 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO 02-17 01:36:54 [loggers.py:257] Engine 000: Avg prompt throughput: 214.1 tokens/s, Avg generation throughput: 122.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.1%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) INFO:     127.0.0.1:50620 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 01:37:00 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO 02-17 01:37:04 [loggers.py:257] Engine 000: Avg prompt throughput: 141.5 tokens/s, Avg generation throughput: 126.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.2%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) INFO 02-17 01:37:14 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 149.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.3%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) INFO:     127.0.0.1:50630 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 01:37:15 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO 02-17 01:37:24 [loggers.py:257] Engine 000: Avg prompt throughput: 256.0 tokens/s, Avg generation throughput: 138.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.3%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) INFO:     127.0.0.1:38792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) INFO 02-17 01:37:34 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 135.6 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) WARNING 02-17 01:37:34 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO 02-17 01:37:44 [loggers.py:257] Engine 000: Avg prompt throughput: 98.8 tokens/s, Avg generation throughput: 140.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.2%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) INFO:     127.0.0.1:44592 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 01:37:46 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO 02-17 01:37:54 [loggers.py:257] Engine 000: Avg prompt throughput: 100.9 tokens/s, Avg generation throughput: 128.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.2%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) INFO 02-17 01:38:04 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 149.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.3%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) INFO 02-17 01:38:14 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 148.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.4%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) INFO:     127.0.0.1:47160 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 01:38:21 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(Worker_TP2 pid=2356194) /users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/model_executor/layers/fla/ops/utils.py:113: UserWarning: Input tensor shape suggests potential format mismatch: seq_len (7) < num_heads (8). This may indicate the inputs were passed in head-first format [B, H, T, ...] when head_first=False was specified. Please verify your input tensor format matches the expected shape [B, T, H, ...].
(Worker_TP2 pid=2356194)   return fn(*contiguous_args, **contiguous_kwargs)
(Worker_TP3 pid=2356195) /users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/model_executor/layers/fla/ops/utils.py:113: UserWarning: Input tensor shape suggests potential format mismatch: seq_len (7) < num_heads (8). This may indicate the inputs were passed in head-first format [B, H, T, ...] when head_first=False was specified. Please verify your input tensor format matches the expected shape [B, T, H, ...].
(Worker_TP3 pid=2356195)   return fn(*contiguous_args, **contiguous_kwargs)
(Worker_TP0 pid=2356192) /users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/model_executor/layers/fla/ops/utils.py:113: UserWarning: Input tensor shape suggests potential format mismatch: seq_len (7) < num_heads (8). This may indicate the inputs were passed in head-first format [B, H, T, ...] when head_first=False was specified. Please verify your input tensor format matches the expected shape [B, T, H, ...].
(Worker_TP0 pid=2356192)   return fn(*contiguous_args, **contiguous_kwargs)
(Worker_TP1 pid=2356193) /users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/model_executor/layers/fla/ops/utils.py:113: UserWarning: Input tensor shape suggests potential format mismatch: seq_len (7) < num_heads (8). This may indicate the inputs were passed in head-first format [B, H, T, ...] when head_first=False was specified. Please verify your input tensor format matches the expected shape [B, T, H, ...].
(Worker_TP1 pid=2356193)   return fn(*contiguous_args, **contiguous_kwargs)
(APIServer pid=2355576) INFO 02-17 01:38:24 [loggers.py:257] Engine 000: Avg prompt throughput: 103.1 tokens/s, Avg generation throughput: 126.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.1%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) INFO:     127.0.0.1:36924 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 01:38:32 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO 02-17 01:38:34 [loggers.py:257] Engine 000: Avg prompt throughput: 224.6 tokens/s, Avg generation throughput: 138.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.2%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) INFO 02-17 01:38:44 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 148.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.3%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) INFO:     127.0.0.1:46264 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 01:38:47 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:33410 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 01:38:51 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO 02-17 01:38:54 [loggers.py:257] Engine 000: Avg prompt throughput: 200.8 tokens/s, Avg generation throughput: 120.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.1%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) INFO:     127.0.0.1:33416 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 01:38:56 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO 02-17 01:39:04 [loggers.py:257] Engine 000: Avg prompt throughput: 932.0 tokens/s, Avg generation throughput: 89.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.7%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) INFO 02-17 01:39:14 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 146.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.8%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) INFO:     127.0.0.1:44604 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 01:39:18 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:44810 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) INFO 02-17 01:39:24 [loggers.py:257] Engine 000: Avg prompt throughput: 83.9 tokens/s, Avg generation throughput: 132.7 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) WARNING 02-17 01:39:24 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO 02-17 01:39:34 [loggers.py:257] Engine 000: Avg prompt throughput: 88.5 tokens/s, Avg generation throughput: 138.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.2%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) INFO:     127.0.0.1:57366 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 01:39:35 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO 02-17 01:39:44 [loggers.py:257] Engine 000: Avg prompt throughput: 194.9 tokens/s, Avg generation throughput: 140.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.2%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) INFO:     127.0.0.1:40488 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 01:39:48 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO 02-17 01:39:54 [loggers.py:257] Engine 000: Avg prompt throughput: 81.3 tokens/s, Avg generation throughput: 126.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.2%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) INFO:     127.0.0.1:53748 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 01:39:54 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:57062 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) INFO 02-17 01:40:04 [loggers.py:257] Engine 000: Avg prompt throughput: 155.7 tokens/s, Avg generation throughput: 129.2 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) WARNING 02-17 01:40:04 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO 02-17 01:40:14 [loggers.py:257] Engine 000: Avg prompt throughput: 110.6 tokens/s, Avg generation throughput: 137.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.2%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) INFO:     127.0.0.1:41730 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 01:40:20 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO 02-17 01:40:24 [loggers.py:257] Engine 000: Avg prompt throughput: 294.3 tokens/s, Avg generation throughput: 135.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.3%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) INFO 02-17 01:40:34 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 148.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.3%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) INFO:     127.0.0.1:49664 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 01:40:44 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO 02-17 01:40:44 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 127.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.1%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) INFO:     127.0.0.1:35328 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 01:40:53 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO 02-17 01:40:54 [loggers.py:257] Engine 000: Avg prompt throughput: 121.4 tokens/s, Avg generation throughput: 138.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.2%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) INFO 02-17 01:41:04 [loggers.py:257] Engine 000: Avg prompt throughput: 190.2 tokens/s, Avg generation throughput: 145.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.3%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) INFO:     127.0.0.1:55268 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 01:41:06 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:35342 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 01:41:13 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO 02-17 01:41:14 [loggers.py:257] Engine 000: Avg prompt throughput: 262.8 tokens/s, Avg generation throughput: 117.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.2%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) INFO:     127.0.0.1:33064 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 01:41:22 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO 02-17 01:41:24 [loggers.py:257] Engine 000: Avg prompt throughput: 78.4 tokens/s, Avg generation throughput: 126.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.1%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) INFO:     127.0.0.1:39564 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 01:41:26 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:39578 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 01:41:29 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:39590 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) INFO 02-17 01:41:34 [loggers.py:257] Engine 000: Avg prompt throughput: 187.5 tokens/s, Avg generation throughput: 115.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) WARNING 02-17 01:41:35 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:44628 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 01:41:38 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:44630 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) INFO 02-17 01:41:44 [loggers.py:257] Engine 000: Avg prompt throughput: 164.7 tokens/s, Avg generation throughput: 97.6 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) WARNING 02-17 01:41:47 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) ERROR 02-17 01:41:47 [serving.py:323] Error in preprocessing prompt inputs
(APIServer pid=2355576) ERROR 02-17 01:41:47 [serving.py:323] Traceback (most recent call last):
(APIServer pid=2355576) ERROR 02-17 01:41:47 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/chat_completion/serving.py", line 301, in render_chat_request
(APIServer pid=2355576) ERROR 02-17 01:41:47 [serving.py:323]     conversation, engine_prompts = await self._preprocess_chat(
(APIServer pid=2355576) ERROR 02-17 01:41:47 [serving.py:323]                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=2355576) ERROR 02-17 01:41:47 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1219, in _preprocess_chat
(APIServer pid=2355576) ERROR 02-17 01:41:47 [serving.py:323]     engine_prompt = await self._tokenize_prompt_input_async(
(APIServer pid=2355576) ERROR 02-17 01:41:47 [serving.py:323]                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=2355576) ERROR 02-17 01:41:47 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1106, in _tokenize_prompt_input_async
(APIServer pid=2355576) ERROR 02-17 01:41:47 [serving.py:323]     async for result in self._tokenize_prompt_inputs_async(
(APIServer pid=2355576) ERROR 02-17 01:41:47 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1127, in _tokenize_prompt_inputs_async
(APIServer pid=2355576) ERROR 02-17 01:41:47 [serving.py:323]     yield await self._normalize_prompt_text_to_input(
(APIServer pid=2355576) ERROR 02-17 01:41:47 [serving.py:323]           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=2355576) ERROR 02-17 01:41:47 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 987, in _normalize_prompt_text_to_input
(APIServer pid=2355576) ERROR 02-17 01:41:47 [serving.py:323]     return self._validate_input(request, input_ids, input_text)
(APIServer pid=2355576) ERROR 02-17 01:41:47 [serving.py:323]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=2355576) ERROR 02-17 01:41:47 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1084, in _validate_input
(APIServer pid=2355576) ERROR 02-17 01:41:47 [serving.py:323]     raise VLLMValidationError(
(APIServer pid=2355576) ERROR 02-17 01:41:47 [serving.py:323] vllm.exceptions.VLLMValidationError: 'max_tokens' or 'max_completion_tokens' is too large: 16384. This model's maximum context length is 32768 tokens and your request has 19904 input tokens (16384 > 32768 - 19904). (parameter=max_tokens, value=16384)
(APIServer pid=2355576) INFO:     127.0.0.1:43168 - "POST /v1/chat/completions HTTP/1.1" 400 Bad Request
(APIServer pid=2355576) WARNING 02-17 01:41:47 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) ERROR 02-17 01:41:47 [serving.py:323] Error in preprocessing prompt inputs
(APIServer pid=2355576) ERROR 02-17 01:41:47 [serving.py:323] Traceback (most recent call last):
(APIServer pid=2355576) ERROR 02-17 01:41:47 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/chat_completion/serving.py", line 301, in render_chat_request
(APIServer pid=2355576) ERROR 02-17 01:41:47 [serving.py:323]     conversation, engine_prompts = await self._preprocess_chat(
(APIServer pid=2355576) ERROR 02-17 01:41:47 [serving.py:323]                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=2355576) ERROR 02-17 01:41:47 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1219, in _preprocess_chat
(APIServer pid=2355576) ERROR 02-17 01:41:47 [serving.py:323]     engine_prompt = await self._tokenize_prompt_input_async(
(APIServer pid=2355576) ERROR 02-17 01:41:47 [serving.py:323]                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=2355576) ERROR 02-17 01:41:47 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1106, in _tokenize_prompt_input_async
(APIServer pid=2355576) ERROR 02-17 01:41:47 [serving.py:323]     async for result in self._tokenize_prompt_inputs_async(
(APIServer pid=2355576) ERROR 02-17 01:41:47 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1127, in _tokenize_prompt_inputs_async
(APIServer pid=2355576) ERROR 02-17 01:41:47 [serving.py:323]     yield await self._normalize_prompt_text_to_input(
(APIServer pid=2355576) ERROR 02-17 01:41:47 [serving.py:323]           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=2355576) ERROR 02-17 01:41:47 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 987, in _normalize_prompt_text_to_input
(APIServer pid=2355576) ERROR 02-17 01:41:47 [serving.py:323]     return self._validate_input(request, input_ids, input_text)
(APIServer pid=2355576) ERROR 02-17 01:41:47 [serving.py:323]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=2355576) ERROR 02-17 01:41:47 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1084, in _validate_input
(APIServer pid=2355576) ERROR 02-17 01:41:47 [serving.py:323]     raise VLLMValidationError(
(APIServer pid=2355576) ERROR 02-17 01:41:47 [serving.py:323] vllm.exceptions.VLLMValidationError: 'max_tokens' or 'max_completion_tokens' is too large: 16384. This model's maximum context length is 32768 tokens and your request has 19904 input tokens (16384 > 32768 - 19904). (parameter=max_tokens, value=16384)
(APIServer pid=2355576) INFO:     127.0.0.1:43176 - "POST /v1/chat/completions HTTP/1.1" 400 Bad Request
(APIServer pid=2355576) WARNING 02-17 01:41:47 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) ERROR 02-17 01:41:47 [serving.py:323] Error in preprocessing prompt inputs
(APIServer pid=2355576) ERROR 02-17 01:41:47 [serving.py:323] Traceback (most recent call last):
(APIServer pid=2355576) ERROR 02-17 01:41:47 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/chat_completion/serving.py", line 301, in render_chat_request
(APIServer pid=2355576) ERROR 02-17 01:41:47 [serving.py:323]     conversation, engine_prompts = await self._preprocess_chat(
(APIServer pid=2355576) ERROR 02-17 01:41:47 [serving.py:323]                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=2355576) ERROR 02-17 01:41:47 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1219, in _preprocess_chat
(APIServer pid=2355576) ERROR 02-17 01:41:47 [serving.py:323]     engine_prompt = await self._tokenize_prompt_input_async(
(APIServer pid=2355576) ERROR 02-17 01:41:47 [serving.py:323]                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=2355576) ERROR 02-17 01:41:47 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1106, in _tokenize_prompt_input_async
(APIServer pid=2355576) ERROR 02-17 01:41:47 [serving.py:323]     async for result in self._tokenize_prompt_inputs_async(
(APIServer pid=2355576) ERROR 02-17 01:41:47 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1127, in _tokenize_prompt_inputs_async
(APIServer pid=2355576) ERROR 02-17 01:41:47 [serving.py:323]     yield await self._normalize_prompt_text_to_input(
(APIServer pid=2355576) ERROR 02-17 01:41:47 [serving.py:323]           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=2355576) ERROR 02-17 01:41:47 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 987, in _normalize_prompt_text_to_input
(APIServer pid=2355576) ERROR 02-17 01:41:47 [serving.py:323]     return self._validate_input(request, input_ids, input_text)
(APIServer pid=2355576) ERROR 02-17 01:41:47 [serving.py:323]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=2355576) ERROR 02-17 01:41:47 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1084, in _validate_input
(APIServer pid=2355576) ERROR 02-17 01:41:47 [serving.py:323]     raise VLLMValidationError(
(APIServer pid=2355576) ERROR 02-17 01:41:47 [serving.py:323] vllm.exceptions.VLLMValidationError: 'max_tokens' or 'max_completion_tokens' is too large: 16384. This model's maximum context length is 32768 tokens and your request has 19904 input tokens (16384 > 32768 - 19904). (parameter=max_tokens, value=16384)
(APIServer pid=2355576) INFO:     127.0.0.1:43186 - "POST /v1/chat/completions HTTP/1.1" 400 Bad Request
(APIServer pid=2355576) WARNING 02-17 01:41:47 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) ERROR 02-17 01:41:47 [serving.py:323] Error in preprocessing prompt inputs
(APIServer pid=2355576) ERROR 02-17 01:41:47 [serving.py:323] Traceback (most recent call last):
(APIServer pid=2355576) ERROR 02-17 01:41:47 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/chat_completion/serving.py", line 301, in render_chat_request
(APIServer pid=2355576) ERROR 02-17 01:41:47 [serving.py:323]     conversation, engine_prompts = await self._preprocess_chat(
(APIServer pid=2355576) ERROR 02-17 01:41:47 [serving.py:323]                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=2355576) ERROR 02-17 01:41:47 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1219, in _preprocess_chat
(APIServer pid=2355576) ERROR 02-17 01:41:47 [serving.py:323]     engine_prompt = await self._tokenize_prompt_input_async(
(APIServer pid=2355576) ERROR 02-17 01:41:47 [serving.py:323]                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=2355576) ERROR 02-17 01:41:47 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1106, in _tokenize_prompt_input_async
(APIServer pid=2355576) ERROR 02-17 01:41:47 [serving.py:323]     async for result in self._tokenize_prompt_inputs_async(
(APIServer pid=2355576) ERROR 02-17 01:41:47 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1127, in _tokenize_prompt_inputs_async
(APIServer pid=2355576) ERROR 02-17 01:41:47 [serving.py:323]     yield await self._normalize_prompt_text_to_input(
(APIServer pid=2355576) ERROR 02-17 01:41:47 [serving.py:323]           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=2355576) ERROR 02-17 01:41:47 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 987, in _normalize_prompt_text_to_input
(APIServer pid=2355576) ERROR 02-17 01:41:47 [serving.py:323]     return self._validate_input(request, input_ids, input_text)
(APIServer pid=2355576) ERROR 02-17 01:41:47 [serving.py:323]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=2355576) ERROR 02-17 01:41:47 [serving.py:323]   File "/users/hideto/miniconda/envs/vLLM_env_update/lib/python3.12/site-packages/vllm/entrypoints/openai/engine/serving.py", line 1084, in _validate_input
(APIServer pid=2355576) ERROR 02-17 01:41:47 [serving.py:323]     raise VLLMValidationError(
(APIServer pid=2355576) ERROR 02-17 01:41:47 [serving.py:323] vllm.exceptions.VLLMValidationError: 'max_tokens' or 'max_completion_tokens' is too large: 16384. This model's maximum context length is 32768 tokens and your request has 19904 input tokens (16384 > 32768 - 19904). (parameter=max_tokens, value=16384)
(APIServer pid=2355576) INFO:     127.0.0.1:43194 - "POST /v1/chat/completions HTTP/1.1" 400 Bad Request
(APIServer pid=2355576) WARNING 02-17 01:41:47 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO:     127.0.0.1:43208 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 01:41:50 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO 02-17 01:41:54 [loggers.py:257] Engine 000: Avg prompt throughput: 607.8 tokens/s, Avg generation throughput: 56.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.2%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) INFO 02-17 01:42:04 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 147.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.3%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) INFO 02-17 01:42:14 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 147.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.4%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) INFO:     127.0.0.1:43214 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 01:42:19 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO 02-17 01:42:24 [loggers.py:257] Engine 000: Avg prompt throughput: 422.7 tokens/s, Avg generation throughput: 127.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.3%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) INFO 02-17 01:42:34 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 145.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.4%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) INFO 02-17 01:42:44 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 145.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.5%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) INFO:     127.0.0.1:33588 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) INFO 02-17 01:42:54 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 128.5 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) WARNING 02-17 01:42:54 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO 02-17 01:43:04 [loggers.py:257] Engine 000: Avg prompt throughput: 516.8 tokens/s, Avg generation throughput: 122.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.4%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) INFO:     127.0.0.1:34852 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 01:43:12 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO 02-17 01:43:14 [loggers.py:257] Engine 000: Avg prompt throughput: 287.0 tokens/s, Avg generation throughput: 133.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.2%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) INFO 02-17 01:43:24 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 146.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.3%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) INFO:     127.0.0.1:38494 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) INFO 02-17 01:43:34 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 130.8 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) WARNING 02-17 01:43:34 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO 02-17 01:43:44 [loggers.py:257] Engine 000: Avg prompt throughput: 751.9 tokens/s, Avg generation throughput: 114.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.6%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) INFO:     127.0.0.1:55354 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 01:43:52 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO 02-17 01:43:54 [loggers.py:257] Engine 000: Avg prompt throughput: 282.0 tokens/s, Avg generation throughput: 133.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.2%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) INFO 02-17 01:44:04 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 147.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.3%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) INFO:     127.0.0.1:37456 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 01:44:13 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO 02-17 01:44:14 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 120.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.2%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) INFO 02-17 01:44:24 [loggers.py:257] Engine 000: Avg prompt throughput: 435.2 tokens/s, Avg generation throughput: 138.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.4%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) INFO 02-17 01:44:34 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 146.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.5%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) INFO 02-17 01:44:44 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 146.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.6%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) INFO 02-17 01:44:54 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 145.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.7%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) INFO 02-17 01:45:04 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 145.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.8%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) INFO 02-17 01:45:14 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 144.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.8%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) INFO:     127.0.0.1:55468 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) WARNING 02-17 01:45:19 [protocol.py:53] The following fields were present in the request but ignored: {'reasoning_max_tokens'}
(APIServer pid=2355576) INFO 02-17 01:45:24 [loggers.py:257] Engine 000: Avg prompt throughput: 979.5 tokens/s, Avg generation throughput: 105.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.7%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) INFO 02-17 01:45:34 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 145.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.8%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) INFO 02-17 01:45:44 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 144.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.8%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) INFO 02-17 01:45:54 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 144.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.9%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) INFO 02-17 01:46:04 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 144.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.0%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) INFO 02-17 01:46:14 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 143.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.1%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) INFO 02-17 01:46:24 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 143.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.2%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) INFO 02-17 01:46:34 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 142.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.3%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) INFO 02-17 01:46:44 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 141.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.4%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) INFO:     127.0.0.1:35172 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=2355576) INFO 02-17 01:46:54 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 105.5 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) INFO 02-17 01:47:04 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
(APIServer pid=2355576) INFO 02-17 02:22:41 [launcher.py:110] Shutting down FastAPI HTTP server.
(Worker_TP0 pid=2356192) INFO 02-17 02:22:41 [multiproc_executor.py:730] Parent process exited, terminating worker
(Worker_TP1 pid=2356193) INFO 02-17 02:22:41 [multiproc_executor.py:730] Parent process exited, terminating worker
(Worker_TP2 pid=2356194) INFO 02-17 02:22:41 [multiproc_executor.py:730] Parent process exited, terminating worker
(Worker_TP3 pid=2356195) INFO 02-17 02:22:41 [multiproc_executor.py:730] Parent process exited, terminating worker
(Worker_TP0 pid=2356192) INFO 02-17 02:22:41 [multiproc_executor.py:774] WorkerProc shutting down.
(Worker_TP1 pid=2356193) INFO 02-17 02:22:41 [multiproc_executor.py:774] WorkerProc shutting down.
(Worker_TP2 pid=2356194) INFO 02-17 02:22:41 [multiproc_executor.py:774] WorkerProc shutting down.
(Worker_TP3 pid=2356195) INFO 02-17 02:22:41 [multiproc_executor.py:774] WorkerProc shutting down.
nanobind: leaked 2 instances!
 - leaked instance 0x145534b56688 of type "xgrammar.xgrammar_bindings.CompiledGrammar"
 - leaked instance 0x145534c237c8 of type "xgrammar.xgrammar_bindings.GrammarMatcher"
nanobind: leaked 6 types!
 - leaked type "xgrammar.xgrammar_bindings.CompiledGrammar"
 - leaked type "xgrammar.xgrammar_bindings.TokenizerInfo"
 - leaked type "xgrammar.xgrammar_bindings.Grammar"
 - leaked type "xgrammar.xgrammar_bindings.GrammarMatcher"
 - leaked type "xgrammar.xgrammar_bindings.BatchGrammarMatcher"
 - leaked type "xgrammar.xgrammar_bindings.GrammarCompiler"
nanobind: leaked 51 functions!
 - leaked function "_debug_print_internal_state"
 - leaked function ""
 - leaked function "from_vocab_and_metadata"
 - leaked function "batch_fill_next_token_bitmask"
 - leaked function "compile_structural_tag"
 - leaked function ""
 - leaked function "__init__"
 - leaked function "serialize_json"
 - leaked function "builtin_json_grammar"
 - leaked function "serialize_json"
 - leaked function "__init__"
 - leaked function "union"
 - leaked function "reset"
 - leaked function "compile_regex"
 - leaked function "compile_json_schema"
 - leaked function "find_jump_forward_string"
 - leaked function "fill_next_token_bitmask"
 - leaked function "from_regex"
 - leaked function "_detect_metadata_from_hf"
 - leaked function "is_terminated"
 - leaked function "rollback"
 - leaked function "__init__"
 - leaked function "batch_accept_string"
 - leaked function "deserialize_json"
 - leaked function ""
 - leaked function ""
 - leaked function "deserialize_json"
 - leaked function ""
 - leaked function "compile_builtin_json_grammar"
 - leaked function "deserialize_json"
 - leaked function ""
 - leaked function "get_cache_size_bytes"
 - leaked function "clear_cache"
 - leaked function "to_string"
 - leaked function "serialize_json"
 - leaked function ""
 - leaked function "__init__"
 - leaked function ""
 - leaked function "from_json_schema"
 - leaked function "accept_token"
 - leaked function "accept_string"
 - leaked function "from_structural_tag"
 - leaked function ""
 - leaked function ""
 - leaked function "compile_grammar"
 - leaked function "batch_accept_token"
 - leaked function "from_ebnf"
 - leaked function ""
 - leaked function ""
 - leaked function "dump_metadata"
 - leaked function "concat"
nanobind: this is likely caused by a reference counting issue in the binding code.
(APIServer pid=2355576) INFO:     Shutting down
(APIServer pid=2355576) INFO:     Waiting for application shutdown.
(APIServer pid=2355576) INFO:     Application shutdown complete.
